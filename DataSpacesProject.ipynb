{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/logo_politecnico.png'>\n",
    "<label>Alberto Benincasa s251415</label>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import delle librerie necessarie per lo svolgimento della tesina\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from scipy import interp\n",
    "from prettytable import PrettyTable\n",
    "from functools import wraps\n",
    "\n",
    "\"\"\"\n",
    "MATPLOTLIB\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "\"\"\"\n",
    "WARNINGS\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import math\n",
    "from scipy.cluster import hierarchy as hc\n",
    "import scipy.spatial as scs\n",
    "import scipy.stats as ss\n",
    "\n",
    "\"\"\"\n",
    "SKLEARN\n",
    "\"\"\"\n",
    "from sklearn.metrics import log_loss, precision_score, recall_score, confusion_matrix, roc_curve, accuracy_score, roc_auc_score, f1_score, auc\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold, GridSearchCV, learning_curve, cross_val_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "\"\"\"\n",
    "PLOTLY\n",
    "\"\"\"\n",
    "import plotly\n",
    "import chart_studio\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "chart_studio.tools.set_credentials_file(username='albertobenincasa', api_key='14CCI1wSA0JGaDZhuouL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Definizione delle costanti che verranno utilizzate nel codice\n",
    "\"\"\"\n",
    "\n",
    "PLOTLY_COLORS = ['#4A708B', '#87CEFA']\n",
    "PLOTLY_OPACITY = 1\n",
    "COLORSCALE_HEATMAP = [         [0.0, '#011f4b'],\n",
    "                [0.1111111111111111, '#03396c'], \n",
    "                [0.2222222222222222, '#005b96'], \n",
    "                [0.3333333333333333, '#2171b5'], \n",
    "                [0.4444444444444444, '#6497b1'], \n",
    "                [0.5555555555555556, '#6baed6'], \n",
    "                [0.6666666666666666, '#B0E2FF'], \n",
    "                [0.7777777777777778, '#b3cde0'], \n",
    "                [0.8888888888888888, '#bdd7e7'], \n",
    "                               [1.0, '#BFEFFF']] \n",
    "COLOR_PALETTE = sns.color_palette(\"Blues\").as_hex()\n",
    "RANDOM_SEED = 11\n",
    "\n",
    "\"\"\"\n",
    "Definizione dei parametri che verranno utilizzati in fase di classificazione\n",
    "\"\"\"\n",
    "\n",
    "LOGISTIC_REGRESSION_PARAMS = [{\n",
    "    'clf__solver': ['liblinear'],\n",
    "    'clf__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'clf__penalty': ['l2', 'l1']\n",
    "},\n",
    "{\n",
    "    'clf__solver': ['newton-cg', 'lbfgs'], \n",
    "    'clf__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'clf__penalty': ['l2']\n",
    "}]\n",
    "SVM_PARAMS = [\n",
    "{\n",
    "    'clf__kernel': ['linear'],\n",
    "    'clf__C': [0.1, 1, 10, 100],\n",
    "}, \n",
    "{\n",
    "    'clf__kernel': ['rbf'],\n",
    "    'clf__C': [0.1, 1, 10, 100],\n",
    "    'clf__gamma': [0.1, 1, 10, 100],\n",
    "}]\n",
    "\n",
    "RANDOM_FOREST_PARAMS = {\n",
    "    'clf__max_depth': [50, 75, 100],\n",
    "    'clf__max_features': [\"sqrt\", \"log2\"],\n",
    "    'clf__criterion': ['gini', 'entropy'],\n",
    "    'clf__n_estimators': [100, 300, 500]\n",
    "}\n",
    "\n",
    "KNN_PARAMS = {\n",
    "    'clf__n_neighbors': [2, 3, 5, 10, 15],\n",
    "    'clf__weights': ['uniform', 'distance'],\n",
    "    'clf__p': [1, 2, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRODUZIONE\n",
    "\n",
    "Il Machine Learning è una branca dell'analisi dati che ha l'obbiettivo di utilizzare questi ultimi per costruire un algoritmo in grado di predirre il comportamento di quelli nuovi. \n",
    "Le tecniche di machine learning sono state una vera rivoluzione nel data mining, in grado di avere un forte impatto sui più svariati campi di applicazione (es: Netflix per i film da consigliare agli utenti, Amazon per consigliare l'acquisto di specifici prodotti agli acquirenti, i diversi assistenti vocali oggi esistenti e persino per la costruzione di macchine a guida autonoma).\n",
    "\n",
    "L'obbiettivo di questa tesina è quello di analizzare un insieme di dati reperibili online per poterne fare, innanzitutto, un processo di pre-analisi, e successivamente confrontare diversi modelli predittivi in modo tale da verificare quale funzioni meglio, sottolineando comunque che i risultati variano molto in base al tipo di dati di cui si è in possesso.\n",
    "\n",
    "Il dataset oggetto di questa analisi, visualizzabile e scaricabile all'indirizzo https://www.kaggle.com/ronitf/heart-disease-uci, consta di una serie di parametri di pazienti affetti o no da disturbi di tipo cardiaco; esso deriva da un database contenente in origine 76 attributi, ma tutti le analisi effettuate si riferiscono ad un sottoinsieme di essi di 14.\n",
    "\n",
    "Con *disturbi di tipo cardiaco* utilizziamo un termine generico per indicare un problema che causa il non corretto funzionamento del cuore. I tre principali e più comuni disturbi sono:\n",
    "* **Coronaropatia**: una qualsiasi alterazione, anatomica o funzionale, delle arterie coronarie, cioè dei vasi sanguigni che portano sangue al muscolo cardiaco.\n",
    "* **Insufficienza cardiaca**:  incapacità del cuore di fornire il sangue in quantità adeguata rispetto all'effettiva richiesta dell'organismo.\n",
    "* **Aritmia**: condizione clinica nella quale viene a mancare la normale frequenza o la regolarità del ritmo cardiaco.\n",
    "\n",
    "Scendendo più nel dettaglio del nostro dataset, apprendiamo che questi 14 attributi specificano diverse caratteristice, di seguito elencate:\n",
    "* **age**: età del paziente in anni\n",
    "* **sex**: sesso del paziente (1=maschio, 0=femmina)\n",
    "* **cp**: tipologia di dolore addominale riscontrata (1=Angina pectoris tipica, 2=Angina pectoris atipica, 3=Dolore non dovuto ad Angina pectoris , 4=Asintomatico)\n",
    "* **trestbps**: pressione del sangue a riposo del paziente (in mmHg all'ammissione in ospedale)\n",
    "* **chol**: colesterolo del paziente misurato in mg/dl\n",
    "* **fbs**: glicemia a digiuno > 120 mg/dl (1=vero; 0=falso) \n",
    "* **restecg**: risutati elettrocardiografici a riposo (0=normale, 1=presenza di anormalità nell'onda ST-T, 2=probabile ipertrofia ventricolo sinistro)  \n",
    "* **thalach**: battito cardiaco massimo raggiunto\n",
    "* **exang**: angina pectoris indotta da esercizio (1=si, 0=no) \n",
    "* **oldpeak**: sottoslivellamento del tratto ST indotta dall'esercizio, non presente a riposo\n",
    "* **slope**: la pendenza durante il picco di esercizio del tratto ST\n",
    "* **ca**: numero di vasi maggiori (0-3) colorati per fluoroscopia\n",
    "* **thal**: malattia del sangue chiamata talassemia (3 = normale; 6 = difetto fisso; 7 = difetto reversibile)\n",
    "* **target**: presenza di malattia (1=si, 0=no)\n",
    "\n",
    "### CARICAMENTO DEL DATASET E ANALISI PRELIMINARI\n",
    "\n",
    "Come prima cosa carichiamo il dataset in una variabile di tipo DataFrame utilizzando la libreria <code>pandas</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carico il dataset presente in locale\n",
    "dataset = pd.read_csv(\"./dataset/heart.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il passo successivo consiste nel guardare il dataset appena caricato in modo da capirne il contenuto, le diverse feature disponibili e il tipo di dato presente in quest'ultime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il dataset ha 303 record e 14 features.\n",
      "\n",
      "Ci sono 2 classi differenti:\n",
      " [1, 0]\n",
      "\n",
      "Valori unici per ogni campo: \n",
      "age          41\n",
      "sex           2\n",
      "cp            4\n",
      "trestbps     49\n",
      "chol        152\n",
      "fbs           2\n",
      "restecg       3\n",
      "thalach      91\n",
      "exang         2\n",
      "oldpeak      40\n",
      "slope         3\n",
      "ca            5\n",
      "thal          4\n",
      "target        2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Dimensione del dataset\n",
    "print(\"Il dataset ha %d record e %d features.\\n\" % dataset.shape)\n",
    "\n",
    "# Conteggio del numero di classi per la classificazione\n",
    "print(f\"Ci sono {dataset['target'].unique().size} classi differenti:\"f\"\\n {dataset['target'].unique().tolist()}\")\n",
    "\n",
    "# Conteggio del numero di valori unici per ogni colonna\n",
    "print(f\"\\nValori unici per ogni campo: \\n{dataset.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING\n",
    "\n",
    "Prima di poter passare alla classificazione, è fondamentale una fase di preprocessing dei dati. \n",
    "Questo è lo step più importante di tutti, perchè pulire i dati prima di inviarli in input ai diversi algoritmi di classificazione ha un duplice vantaggio: per prima cosa, evitiamo di sovraccaricare la potenza di calcolo della macchina per effettuare operazioni che poi con tutta probabilità risulteranno ininfluenti per la classificazione finale, e per seconda, analizzare i dati prima di processarli ci permette di ottenere la classificazione migliore possibile.\n",
    "\n",
    "Inoltre, è molto importante anche controllare la presenza di eventuali valori nulli: nel caso in cui ci fossero colonne non valorizzate, è bene procedere con l'eliminazione di quest'ultime per ridurre il numero di dati su cui dobbiamo lavorare.\n",
    "\n",
    "La strategia alternativa all'eliminazione di intere colonne, particolarmente vantaggiosa solo se tutti i valori (o perlomeno, la maggior parte) sono non definiti è quella di inserire valori di default (come 0) per i record che non hanno valori definiti.\n",
    "\n",
    "Come possiamo notare, non vi è traccia della presenza di valori nulli nel dataset in analisi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conteggio dei valori NaN: \n",
      "age         0\n",
      "sex         0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalach     0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          0\n",
      "thal        0\n",
      "target      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Controllo la presenza di valori nulli\n",
    "print(f\"\\nConteggio dei valori NaN: \\n{dataset.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La prima, semplice, analisi da fare è quella di vedere la tipologia dei dati presenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come possiamo notare, tutti i nostri attributi sono di tipo numerico. Nonostante questo, dalla descrizione del dataset sappiamo che alcuni di questi valori corrispondono in realtà ad elementi di tipo categorico. \n",
    "\n",
    "Infatti, i campi valorizzati con delle stringhe non permettono un'analisi semplice ed immediata, [in quanto sarebbe corretto encodare PARLARE DI QUESTO]\n",
    "\n",
    "Vediamo ora come i nostri campioni sono suddivisi nelle diverse classi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    165\n",
      "0    138\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y = dataset['target'].value_counts()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunatamente, il dataset è abbastanza bilanciato: abbiamo circa lo stesso numero di campioni classificati come malati e non malati. Questo è molto importante perchè possiamo assegnare lo stesso peso alle due classi quando passeremo alla fase di classificazione.\n",
    "\n",
    "Inoltre, possiamo notare come il nostro compito di classificazione sarà di tipo binario: infatti, il nostro obiettivo è riuscire a predire se un paziente potrebbe riscontrare malattie cardiache oppure no. Possiamo ora approfondire alcuni dettagli statistici sul dataset utilizzando il metodo <code>describe()</code> sul nostro DataFrame di pandas. L'output mostrerà:\n",
    "* **count** $\\to$ specifica il numero dei record presenti nel dataset \n",
    "* **mean** $\\to$ specifica la media dell'attributo calcolata per tutti i record\n",
    "* **std** $\\to$ specifica la deviazione standard dell'attributo\n",
    "* **min** $\\to$ specifica il valore minimo dell'attributo\n",
    "* **25%** $\\to$ il 25% dei record ha un valore minore di quello visualizzato (lower percentile)\n",
    "* **50%** $\\to$ il 50% dei record ha un valore minore di quello visualizzato (median percentile)\n",
    "* **75%** $\\to$ il 75% dei record ha un valore minore di quello visualizzato (upper percentile)\n",
    "* **max** $\\to$ specifica il valore massimo dell'attributo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>54.366337</td>\n",
       "      <td>0.683168</td>\n",
       "      <td>0.966997</td>\n",
       "      <td>131.623762</td>\n",
       "      <td>246.264026</td>\n",
       "      <td>0.148515</td>\n",
       "      <td>0.528053</td>\n",
       "      <td>149.646865</td>\n",
       "      <td>0.326733</td>\n",
       "      <td>1.039604</td>\n",
       "      <td>1.399340</td>\n",
       "      <td>0.729373</td>\n",
       "      <td>2.313531</td>\n",
       "      <td>0.544554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.082101</td>\n",
       "      <td>0.466011</td>\n",
       "      <td>1.032052</td>\n",
       "      <td>17.538143</td>\n",
       "      <td>51.830751</td>\n",
       "      <td>0.356198</td>\n",
       "      <td>0.525860</td>\n",
       "      <td>22.905161</td>\n",
       "      <td>0.469794</td>\n",
       "      <td>1.161075</td>\n",
       "      <td>0.616226</td>\n",
       "      <td>1.022606</td>\n",
       "      <td>0.612277</td>\n",
       "      <td>0.498835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>47.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>274.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age         sex          cp    trestbps        chol         fbs  \\\n",
       "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
       "mean    54.366337    0.683168    0.966997  131.623762  246.264026    0.148515   \n",
       "std      9.082101    0.466011    1.032052   17.538143   51.830751    0.356198   \n",
       "min     29.000000    0.000000    0.000000   94.000000  126.000000    0.000000   \n",
       "25%     47.500000    0.000000    0.000000  120.000000  211.000000    0.000000   \n",
       "50%     55.000000    1.000000    1.000000  130.000000  240.000000    0.000000   \n",
       "75%     61.000000    1.000000    2.000000  140.000000  274.500000    0.000000   \n",
       "max     77.000000    1.000000    3.000000  200.000000  564.000000    1.000000   \n",
       "\n",
       "          restecg     thalach       exang     oldpeak       slope          ca  \\\n",
       "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
       "mean     0.528053  149.646865    0.326733    1.039604    1.399340    0.729373   \n",
       "std      0.525860   22.905161    0.469794    1.161075    0.616226    1.022606   \n",
       "min      0.000000   71.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000  133.500000    0.000000    0.000000    1.000000    0.000000   \n",
       "50%      1.000000  153.000000    0.000000    0.800000    1.000000    0.000000   \n",
       "75%      1.000000  166.000000    1.000000    1.600000    2.000000    1.000000   \n",
       "max      2.000000  202.000000    1.000000    6.200000    2.000000    4.000000   \n",
       "\n",
       "             thal      target  \n",
       "count  303.000000  303.000000  \n",
       "mean     2.313531    0.544554  \n",
       "std      0.612277    0.498835  \n",
       "min      0.000000    0.000000  \n",
       "25%      2.000000    0.000000  \n",
       "50%      2.000000    1.000000  \n",
       "75%      3.000000    1.000000  \n",
       "max      3.000000    1.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"525px\"\n",
       "            src=\"https://plot.ly/~albertobenincasa/0.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f8325898b00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_bar(type, data, col, visible=False):\n",
    "    \"\"\"\n",
    "    Creazione di un istogramma   \n",
    "    \"\"\"    \n",
    "    \n",
    "    if type == \"sick\":\n",
    "        c = PLOTLY_COLORS[0]\n",
    "    else:\n",
    "        c = PLOTLY_COLORS[1]\n",
    "    \n",
    "    return go.Histogram(\n",
    "        x = data[col],\n",
    "        name = type,\n",
    "        marker = dict(color = c),\n",
    "        visible = visible,\n",
    "        opacity = PLOTLY_OPACITY,\n",
    "    )\n",
    "\n",
    "def feature_histogram(data, feature, title):\n",
    "    \"\"\"\n",
    "    Stampa l'istogramma della distribuzione del valore per una funzione\n",
    "    \n",
    "    :param (DataFrame) data: il dataset\n",
    "    :param (string) feature: attributo che vogliamo visualizzare\n",
    "    :param (string) title: titolo del grafico\n",
    "    \"\"\"\n",
    "    \n",
    "    trace1 = create_bar(\"sick\", data[data['target'] == 1], feature, True)\n",
    "    trace2 = create_bar(\"healthy\", data[data['target'] == 0], feature, True)\n",
    "    \n",
    "    data = [trace1, trace2]\n",
    "    \n",
    "    layout = dict(\n",
    "        title=title,\n",
    "        autosize=True,\n",
    "        yaxis=dict(\n",
    "            title='value',\n",
    "            automargin=True,\n",
    "        ),\n",
    "        legend=dict(\n",
    "            x=0,\n",
    "            y=1,\n",
    "        ),\n",
    "        barmode='group',\n",
    "        bargap=0.15,\n",
    "        bargroupgap=0.1\n",
    "    )\n",
    "    fig = dict(data=data, layout=layout)\n",
    "\n",
    "    return py.iplot(fig, filename=title)\n",
    "\n",
    "feature_histogram(dataset, 'target', 'Distribuzione classi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto possiamo analizzare la distribuzione dei nostri dati usando dei boxplot: un boxplot è un modo standardizzato per visualizzare la distribuzione dei dati sulla base di un riepilogo di cinque numeri (min, lower percentile, median, upper percentile, max); risulta molto utile per visualizzare dati anomali e per controllare se i dati sono ditribuiti in modo simmetrico, quanto sono aggregati.\n",
    "\n",
    "Le informazioni che possiamo ricavare da un box plot sono:\n",
    "* **median**(Q2 / 50esimo percentile) $\\to$ il valore medio del dataset\n",
    "* **first quartile**(Q1 / 25esimo percentile) $\\to$ il valore medio tra il valore più piccolo (non il minimo) e il median del dataset\n",
    "* **third quartile**(Q3 / 75esimo percentile) $\\to$ il valore medio tra il valore più grande (non il massimo) e il median del dataset\n",
    "* **interquartile range**(IQR) $\\to$ dal 25esimo al 75esimo percentile\n",
    "* **outliers** $\\to$ elementi visualizzati come singoli punti\n",
    "* **maximum** $\\to$ Q3 + 1.5xIQR\n",
    "* **minimum** $\\to$ Q1 - 1.5xIQR\n",
    "\n",
    "Chiaramente, ha poco senso visualizzare in questo tipo di grafici quegli attributi che sono binari o che presentano poche opzioni, per cui filtreremo questi dati prima di visualizzare i grafici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"525px\"\n",
       "            src=\"https://plot.ly/~albertobenincasa/2.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f83254e00b8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_box(type, data, col, visible=False):\n",
    "    \"\"\"\n",
    "    Creazione di una box\n",
    "    \"\"\"   \n",
    "    if type == 'sick':\n",
    "        c = PLOTLY_COLORS[0]\n",
    "    else:\n",
    "        c = PLOTLY_COLORS[1]\n",
    "        \n",
    "    return go.Box(\n",
    "        y = data[col],\n",
    "        name = type,\n",
    "        marker = dict(color = c),\n",
    "        visible = visible,\n",
    "        opacity = PLOTLY_OPACITY,\n",
    "    )\n",
    "\n",
    "sicks = dataset[dataset['target'] == 1]\n",
    "healthy = dataset[dataset['target'] == 0]\n",
    "box_features = [col for col in dataset.columns if ((col != 'class') and (dataset[col].nunique() > 5))]\n",
    "\n",
    "active_index = 0\n",
    "\n",
    "box_sick = [(create_box('sick', sicks, col, False) if i != active_index\n",
    "           else create_box('sick', sicks, col, True))\n",
    "             for i, col in enumerate(box_features)\n",
    "            ]\n",
    "box_healthy = [(create_box('healthy', healthy, col, False) if i != active_index\n",
    "            else create_box('healthy', healthy, col, True))\n",
    "             for i, col in enumerate(box_features)\n",
    "            ]\n",
    "\n",
    "data = box_sick + box_healthy\n",
    "number_of_features = len(box_features)\n",
    "steps = []\n",
    "\n",
    "for i in range(number_of_features):\n",
    "    step = dict(\n",
    "        method = 'restyle',  \n",
    "        args = ['visible', [False] * number_of_features],\n",
    "        label = box_features[i],\n",
    "    )\n",
    "    step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n",
    "    steps.append(step)\n",
    "    \n",
    "sliders = [dict(\n",
    "    active = active_index,\n",
    "    currentvalue = dict(\n",
    "        prefix = \"Feature: \", \n",
    "        xanchor= 'center',\n",
    "    ),\n",
    "    pad = {\"t\": 50},\n",
    "    steps = steps,\n",
    "    len=1,\n",
    ")]\n",
    "\n",
    "layout = dict(\n",
    "    sliders=sliders,\n",
    "    autosize=True,\n",
    "    yaxis=dict(\n",
    "        title='valori',\n",
    "        automargin=True,\n",
    "    ),\n",
    "    legend=dict(\n",
    "        x=0,\n",
    "        y=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "py.iplot(fig, filename='box_slider')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "commenti sul grafico sopra\n",
    "\n",
    "Gli Istogrammi sono un tipo di grafico molto utile alla rappresentazione di variabili di tipo categorico che si presenta con delle barre rettangolari la cui altezza è proporzionale ai valori che rappresentano.\n",
    "Con uno slider possiamo muoverci lungo le diverse features, per visualizzare meglio la distribuzione dei diversi valori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"525px\"\n",
       "            src=\"https://plot.ly/~albertobenincasa/4.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f83255487f0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_features = [col for col in dataset.columns if (col != 'target')]\n",
    "\n",
    "active_index = 0\n",
    "hist_sick = [(create_bar('sick', sicks, col, False) if i != active_index\n",
    "           else create_bar('sick', sicks, col, True))\n",
    "             for i, col in enumerate(hist_features)\n",
    "            ]\n",
    "hist_healthy = [(create_bar('healthy', healthy, col, False) if i != active_index\n",
    "           else create_bar('healthy', healthy, col, True))\n",
    "             for i, col in enumerate(hist_features)\n",
    "            ]\n",
    "\n",
    "total_data = hist_sick + hist_healthy\n",
    "number_of_features = len(hist_features)\n",
    "steps = []\n",
    "\n",
    "for i in range(number_of_features):\n",
    "    step = dict(\n",
    "        method = 'restyle',\n",
    "        args = ['visible', [False] * number_of_features],\n",
    "        label = hist_features[i],\n",
    "    )\n",
    "    step['args'][1][i] = True\n",
    "    steps.append(step)\n",
    "    \n",
    "sliders = [dict(\n",
    "    active = active_index,\n",
    "    currentvalue = dict(\n",
    "        prefix = \"Feature: \", \n",
    "        xanchor= 'center',\n",
    "    ),\n",
    "    pad = {\"t\": 50},\n",
    "    steps = steps,\n",
    "    len=1,\n",
    ")]\n",
    "\n",
    "layout = dict(\n",
    "    sliders=sliders,\n",
    "    autosize=True,\n",
    "    yaxis=dict(\n",
    "        title='valori',\n",
    "        automargin=True,\n",
    "    ),\n",
    "    legend=dict(\n",
    "        x=0,\n",
    "        y=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig = dict(data=total_data, layout=layout)\n",
    "py.iplot(fig, filename='bar_slider')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come possiamo notare dal grafico soprastante, valori come *age*, *trestbs* e *chol* non sono troppo determinanti per la classificazione. Altri attributi come *cp*, *thal*, *slope* e *thalach* hanno valori tutto sommato ben distinti tra le due classi: questi saranno gli attributi che saranno più importanti nella fase di classificazione.\n",
    "\n",
    "Una matrice di correlazione è una tabella che mostra i coefficienti di correlazione tra insiemi di variabili. \n",
    "Ogni variabile casuale ($X_i$) nella tabella è correlata con ciascuno degli altri valori nella tabella ($X_j$); questo permette di vedere quali coppie hanno la più alta correlazione.\n",
    "La correlazione si riferisce a qualsiasi associazione statistica, ma nell'uso comune del termine si indica quanto due variabili siano vicine ad avere una relazione lineare l'una con l'altra.\n",
    "\n",
    "In questo frangente useremo la correlazione di Pearson, che è una misura della correlazione lineare tra due variabili $X$ e $Y$. Secondo la diseguaglianza di Cauchy-Schwartz, tale valore è compreso tra +1 e -1, dove +1 è la correlazione positiva totale, 0 è nessuna correlazione lineare e -1 è una correlazione negativa totale.\n",
    "\n",
    "Questo coefficiente è calcolato come $ \\frac{cov(X, Y)}{\\sigma_X\\sigma_Y} $ dove:\n",
    "* $cov$ si chiama covarianza ed è calcolata come ($\\mu$ è il valore medio)  $$ cov(X, Y) = E[(X - \\mu_X)(Y - \\mu_Y)] $$\n",
    "* $\\sigma_X$ è la deviazione standard di $X$ $$ \\sigma_X = \\sqrt{E[X^2] - (E[X])^2} $$\n",
    "* $\\sigma_Y$ è la deviazione standard di $Y$ $$ \\sigma_Y = \\sqrt{E[Y^2] - (E[Y])^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(matrix, title):\n",
    "\n",
    "    z_text = np.around(matrix.values.tolist(), decimals=2)\n",
    "\n",
    "    figure = ff.create_annotated_heatmap(z=matrix.values, \n",
    "                                         x=matrix.columns.tolist(), \n",
    "                                         y=matrix.index.tolist(),\n",
    "                                         annotation_text=z_text,\n",
    "                                         colorscale=COLORSCALE_HEATMAP,\n",
    "                                         showscale=True)\n",
    "\n",
    "    figure.layout.title = title\n",
    "    figure.layout.autosize = False\n",
    "    figure.layout.width = 850\n",
    "    figure.layout.height = 850\n",
    "    figure.layout.margin = go.layout.Margin(l=140, r=100, b=200, t=80)\n",
    "    figure.layout.xaxis.update(side='bottom')\n",
    "    figure.layout.yaxis.update(side='left')\n",
    "\n",
    "    for i in range(len(figure.layout.annotations)):\n",
    "        figure.layout.annotations[i].font.size = 8\n",
    "                                    \n",
    "    return py.iplot(figure, filename=title)\n",
    "\n",
    "def plot_correlation_row(matrix, title):\n",
    "    \n",
    "    matrix = pd.Series.to_frame(matrix.loc['class']).transpose()\n",
    "    z_text = np.around(matrix.values.tolist(), decimals=2)\n",
    "\n",
    "    figure = ff.create_annotated_heatmap(z=matrix.values, \n",
    "                                         x=matrix.columns.tolist(), \n",
    "                                         y=matrix.index.tolist(),\n",
    "                                         annotation_text=z_text,\n",
    "                                         colorscale=COLORSCALE_HEATMAP,\n",
    "                                         showscale=False)\n",
    "\n",
    "    figure.layout.title = title\n",
    "    figure.layout.autosize = False\n",
    "    figure.layout.width = 850\n",
    "    figure.layout.height = 220\n",
    "    figure.layout.xaxis.update(side='bottom')\n",
    "    figure.layout.yaxis.update(side='left')\n",
    "\n",
    "    for i in range(len(figure.layout.annotations)):\n",
    "        figure.layout.annotations[i].font.size = 8\n",
    "\n",
    "\n",
    "    return py.iplot(figure, filename=title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"850px\"\n",
       "            height=\"850px\"\n",
       "            src=\"https://plot.ly/~albertobenincasa/6.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f832701c198>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_dataset = dataset.corr(method='pearson')\n",
    "plot_correlation_matrix(corr_dataset, 'Matrice di correlazione')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardando ai valori della matrice di correlazione riportata sopra, possiamo notare facilmente che gli attributi presenti nel nostro dataset non sono molto correlati.\n",
    "\n",
    "Ciò implica che dobbiamo includere tutte le features, in quanto possiamo e conviene eliminare solo quelle in cui la correlazione di due o più features è molto elevata.\n",
    "\n",
    "Considerando però l'attributo *target*, che ricordiamo specifica se il paziente in analisi è affetto oppure no da qualche disturbo cardiaco, notiamo che i fattori principali che ne discriminano il valore sono le features *cp (chest pain)*, *thalach* e *slope*; ciò l'avevamo ipotizzato anche guardando l'istogramma, per cui ciò ne conferma ulteriormente l'importanza.\n",
    "\n",
    "Un dendrogramma è un diagramma che rappresenta un albero. Questa rappresentazione schematica viene utilizzata in contesti diversi, ma vedremo il caso in cui si rappresenta il clustering gerarchico.\n",
    "Essa illustra la disposizione dei cluster e l'obiettivo è quello di analizzare se abbiamo caratteristiche duplicate. Questo rientra sempre nell'analisi in vista di una possibile riduzione della dimensionalità dei dati.\n",
    "\n",
    "Il criterio di collegamento determina la distanza tra insiemi di osservazioni come una funzione della distanza a coppie tra osservazioni (The linkage criterion determines the distance between sets of observations as a function of the pairwise distances between observations). Useremo UPGMA(Unweighted Pair Group Method With Arithmetic Mean), per cui la prossimità tra due cluster viene calcolata come la media aritmetica .. (Proximity between two clusters is the arithmetic mean of all the proximities between the objects of one, on one side, and the objects of the other, on the other side. The method is frequently set the default one in hierarhical clustering packages.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800px\"\n",
       "            height=\"600px\"\n",
       "            src=\"https://plot.ly/~albertobenincasa/12.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f832a2422b0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = dataset.columns\n",
    "inverse_correlation = 1 - corr_dataset # This is the 'dissimilarity' method\n",
    "\n",
    "fig = ff.create_dendrogram(inverse_correlation, \n",
    "                           labels=names, \n",
    "                           colorscale=COLOR_PALETTE,\n",
    "                           linkagefun=lambda x: hc.linkage(x, 'average'))\n",
    "\n",
    "fig['layout'].update(dict(\n",
    "    title=\"Dendrogramma di correlazione tra gli attributi\",\n",
    "    width=800, \n",
    "    height=600,\n",
    "    xaxis=dict(\n",
    "        title='Features',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Distance',\n",
    "    ),\n",
    "))\n",
    "py.iplot(fig, filename='dendrogram_corr_clustering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come vediamo dal grafico sopra, non abbiamo distanze molto piccoli tra i clusters, e ciò, insieme al precedente grafico della matrice di correlazione, ci rafforza l'idea che tenere tutti gli attributi in fase di classificazione è la strada giusta da prendere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NORMALIZZAZIONE DEI DATI\n",
    "\n",
    "La maggior parte delle volte, i set di dati contengono caratteristiche molto variabili in termini di grandezze, unità di misura e range. \n",
    "Poichè la maggior parte degli algoritmi di machine learning utilizza la distanza euclidea per misurare la distanza tra due dati nei loro calcoli, questo potrebbe causare dei problemi.\n",
    "\n",
    "Se lasciati così come sono, questi algoritmi prendono solo la grandezza delle funzioni che trascurano le unità: i risultati variano molto tra le diverse unità, ad esempio tra 5Kg e 5000g.\n",
    "\n",
    "Per questo, le caratteristiche con grandezze elevate peseranno molto di più nei calcoli rispetto agli attributi con minor grandezza. Per risolvere questo problema, dobbiamo portare tutti gli attributi sullo stesso livello di grandezza e ciò si può fare con il ridimensionamento.\n",
    "\n",
    "Per fare ciò, useremo lo StandardScaler, che standardizza i nostri dati sia con la media che con la deviazione standard. L'operazione matematica eseguita sarà: $$ x' = \\frac{x - \\mu_x}{\\sigma_x} $$\n",
    "Dopodichè, spezzeremo il nostro dataset in un array di dati non classificati e un array di label, che utilizzeremo nella fase di classificazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dataframe_to_array(data):\n",
    "    X_data = data.drop(['target'], axis=1)\n",
    "    y_data = data['target']\n",
    "    return X_data, y_data\n",
    "\n",
    "def scale_data(X_data):\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True, copy=True)\n",
    "    return scaler.fit_transform(X_data)\n",
    "\n",
    "X_data, y_data = dataframe_to_array(dataset)\n",
    "X_scaled_data = scale_data(X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRINCIPAL COMPONENT ANALYSIS\n",
    "\n",
    "Quando i nostri dati sono rappresentati da una matrice troppo grande (ossia il numero di dimensioni è troppo alto) risulta difficile estrarre le caratteristiche più interessanti e trovare correlazioni tra di loro, senza considerare che lo spazio occupato è più alto del necessario.\n",
    "La PCA è una tecnica che consente di ridurre la dimensionalità dei dati preservando le differenze più importanti che interccorrono tra i campioni.\n",
    "\n",
    "Questa trasformazione è definita in modo tale che la prima componente principale abbia la varianza più grande possibile (ovvero, tiene conto il più possibile della variabilità dei valori assunti da una variabile) e ciascun componente successivo ha a sua volta la varianza più alta possibile sotto il vincolo di essere ortogonale alle componenti precedenti.\n",
    "I vettori risultanti (essendo ciascuno una combinazione lineare delle variabili e contenente N osservazioni) costituiscono una base ortogonale.\n",
    "\n",
    "**Componente Principale #1** $\\to$ punta nella direzione della maggior varianza\n",
    "\n",
    "**Ogni successiva Componente Principale** $\\to$ ortogonale alle precedenti che punta nella direzione della maggior varianza dello spazio rimanente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_variance(pca, title):\n",
    "    \n",
    "    \"\"\"\n",
    "    Grafico della varianza cumulativa di tutte le PC\n",
    "    \n",
    "    :param (pca_object) pca: oggetto PCA\n",
    "    :param (string) title: titolo del grafico\n",
    "    \"\"\"   \n",
    "\n",
    "    tot_var = np.sum(pca.explained_variance_)\n",
    "    ex_var = [(i / tot_var) * 100 for i in sorted(pca.explained_variance_, reverse=True)]\n",
    "    cum_ex_var = np.cumsum(ex_var)\n",
    "\n",
    "    cum_var_bar = go.Bar(\n",
    "        x=list(range(1, len(cum_ex_var) + 1)), \n",
    "        y=ex_var,\n",
    "        name=\"Varianza di ogni componente\",\n",
    "        marker=dict(\n",
    "            color=PLOTLY_COLORS[0],\n",
    "        ),\n",
    "        opacity=PLOTLY_OPACITY\n",
    "        )\n",
    "\n",
    "    variance_line = go.Scatter(\n",
    "        x=list(range(1, len(cum_ex_var) + 1)),\n",
    "        y=cum_ex_var,\n",
    "        mode='lines+markers',\n",
    "        name=\"Varianza cumulativa\",\n",
    "        marker=dict(\n",
    "            color=PLOTLY_COLORS[1],\n",
    "        ),\n",
    "        opacity=PLOTLY_OPACITY,\n",
    "        line=dict(\n",
    "            shape='hv',\n",
    "        ))\n",
    "    data = [cum_var_bar, variance_line]\n",
    "    layout = go.Layout(\n",
    "        autosize=True,\n",
    "        title=title,\n",
    "        yaxis=dict(\n",
    "            title='Varianza (%)',\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=\"Componenti principali\",\n",
    "            dtick=1,\n",
    "            rangemode='nonnegative'\n",
    "        ),\n",
    "        legend=dict(\n",
    "            x=0,\n",
    "            y=1,\n",
    "        ),\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    return py.iplot(fig, filename=title)\n",
    "\n",
    "def compress_data(X_dataset, n_components, plot_comp=False, graph_title=''):\n",
    "    \n",
    "    \"\"\"\n",
    "    PCA reduction di un insieme di dati\n",
    "\n",
    "    :param (array of arrays) X_dataset: Dataset da ridurre\n",
    "    :param (int) n_components: N componenti da proiettare\n",
    "    :param (bool) plot_comp: Disegno della varianza cumulativa\n",
    "\n",
    "    :returns (pandas dataframe) X_df_reduced: Dataframe contenente il dataset ridotto\n",
    "    :return (iplot) p: Visualizzare grafico, solo se plot_com vale True.\n",
    "    \"\"\"   \n",
    "\n",
    "    pca = PCA(random_state=11) #random seed\n",
    "    projected_data = pca.fit_transform(X_dataset)\n",
    "\n",
    "    if plot_comp:\n",
    "        p = plot_cumulative_variance(pca, graph_title)\n",
    "\n",
    "    pca.components_ = pca.components_[:n_components]\n",
    "    reduced_data = np.dot(projected_data, pca.components_.T)\n",
    "    X_df_reduced = pd.DataFrame(reduced_data, columns=[\"PC#%d\" % (x + 1) for x in range(n_components)])\n",
    "    if plot_comp:\n",
    "        return p, X_df_reduced\n",
    "    else:\n",
    "        return X_df_reduced\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"525px\"\n",
       "            src=\"https://plot.ly/~albertobenincasa/18.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f83269582b0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot, X_df_ohc_reduced = compress_data(X_dataset=X_scaled_data,\n",
    "              n_components=8,\n",
    "              plot_comp=True,\n",
    "              graph_title=\"Varianza individuale e cumulativa\")\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC#1</th>\n",
       "      <th>PC#2</th>\n",
       "      <th>PC#3</th>\n",
       "      <th>PC#4</th>\n",
       "      <th>PC#5</th>\n",
       "      <th>PC#6</th>\n",
       "      <th>PC#7</th>\n",
       "      <th>PC#8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.674659</td>\n",
       "      <td>2.095604</td>\n",
       "      <td>3.809485</td>\n",
       "      <td>0.111267</td>\n",
       "      <td>1.151868</td>\n",
       "      <td>-1.089498</td>\n",
       "      <td>0.224776</td>\n",
       "      <td>-0.555006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.732016</td>\n",
       "      <td>1.850302</td>\n",
       "      <td>-0.558395</td>\n",
       "      <td>-0.096317</td>\n",
       "      <td>2.620567</td>\n",
       "      <td>-1.278350</td>\n",
       "      <td>0.989618</td>\n",
       "      <td>-0.101235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.523563</td>\n",
       "      <td>-0.160394</td>\n",
       "      <td>-0.290459</td>\n",
       "      <td>-0.790662</td>\n",
       "      <td>-0.092465</td>\n",
       "      <td>1.432148</td>\n",
       "      <td>1.631711</td>\n",
       "      <td>0.391299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.181329</td>\n",
       "      <td>-0.676779</td>\n",
       "      <td>0.040449</td>\n",
       "      <td>0.579549</td>\n",
       "      <td>0.353748</td>\n",
       "      <td>-0.006323</td>\n",
       "      <td>-0.480461</td>\n",
       "      <td>0.870860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PC#1      PC#2      PC#3      PC#4      PC#5      PC#6      PC#7  \\\n",
       "0  0.674659  2.095604  3.809485  0.111267  1.151868 -1.089498  0.224776   \n",
       "1  0.732016  1.850302 -0.558395 -0.096317  2.620567 -1.278350  0.989618   \n",
       "2 -0.523563 -0.160394 -0.290459 -0.790662 -0.092465  1.432148  1.631711   \n",
       "3  0.181329 -0.676779  0.040449  0.579549  0.353748 -0.006323 -0.480461   \n",
       "\n",
       "       PC#8  \n",
       "0 -0.555006  \n",
       "1 -0.101235  \n",
       "2  0.391299  \n",
       "3  0.870860  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df_ohc_reduced.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cercheremo ora di utilizzare uno scatter-plot per mostrare se un algoritmo di clustering applicato sulle prime due componenti principali è in grado di separare i campioni in due cluster diversi.\n",
    "\n",
    "PCA cerca di trovare combinazioni di features che conducono alla massima separazione tra i dati; ciò significa che, se avessimo una dimensione nel nostro dataset che rimane la stessa per tutti i dati, questa non verrebbe considerata, da solo o come combinazione, tra le componenti principali. Solo le caratteristiche che variano molto da dato a dato fanno parte di esse. Di conseguenza, i punti dovrebbero apparire abbastanza distanti l'uno dall'altro sul grafico.\n",
    "\n",
    "Il grafico potrebbe avere anche poco senso, e ciò può avvenire per due fattori principali \n",
    "1. E' presente molta varianza nel dataset, e perciò le prime due componenti non bastano a rappresentare significativamente i dati\n",
    "2. L'algoritmo di clustering si focalizza su features che non sono considerate importanti da PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"525px\"\n",
       "            src=\"https://plot.ly/~albertobenincasa/16.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f832675fd68>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = X_scaled_data\n",
    "pca = PCA(n_components=2)\n",
    "x = pca.fit_transform(values)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=11)\n",
    "X_clustered = kmeans.fit_predict(values)\n",
    "\n",
    "c1_idx = np.where(X_clustered == 0)\n",
    "c2_idx = np.where(X_clustered == 1)\n",
    "\n",
    "p1 = go.Scatter(\n",
    "    x=np.take(x[:,0], indices=c1_idx)[0],\n",
    "    y=np.take(x[:,1], indices=c1_idx)[0],\n",
    "    mode='markers',\n",
    "    name=\"Cluster1\",\n",
    "    marker=dict(\n",
    "        color=COLOR_PALETTE[2],\n",
    "    ),\n",
    "    opacity=PLOTLY_OPACITY)\n",
    "\n",
    "p2 = go.Scatter(\n",
    "    x=np.take(x[:,0], indices=c2_idx)[0],\n",
    "    y=np.take(x[:,1], indices=c2_idx)[0],\n",
    "    mode='markers',\n",
    "    name=\"Cluster2\",\n",
    "    marker=dict(\n",
    "        color=COLOR_PALETTE[5],\n",
    "    ),\n",
    "    opacity=PLOTLY_OPACITY)\n",
    "\n",
    "data = [p1, p2]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Dati clusterizzati usando le prime due componenti',\n",
    "    autosize=True,\n",
    "    yaxis=dict(\n",
    "        title='Seconda componente',\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Prima componente\",\n",
    "        dtick=1,\n",
    "    ),\n",
    "    legend=dict(\n",
    "        x=0,\n",
    "        y=1,\n",
    "    ),\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='clusters-scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASSIFICAZIONE\n",
    "\n",
    "In questa fase esploreremo diversi metodi di supervised learning e vedremo alla fine quale di questi classifica meglio i nostri dati.\n",
    "\n",
    "Prima di iniziare, vediamo quale tipo di pre-processed data è meglio utilizzare. Siccome il nostro dataset è particolarmente piccolo, sicuramente la riduzione della dimensionalità vista poco sopra con PCA non è necessaria.\n",
    "\n",
    "La prima cosa da fare è suddividere i dati:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled_data,y_data, test_size=0.2, random_state=RANDOM_SEED, stratify=y_data)\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_df_ohc_reduced,y_data, test_size=0.2, random_state=RANDOM_SEED, stratify=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gridcv_scores(grid_search, n=5):\n",
    "    \"\"\"\n",
    "    Stampa il punteggio migliore raggiunto da grid_search, con i relativi parametri\n",
    "\n",
    "    :param (estimator) clf: oggetto con all'interno un classificatore\n",
    "    :param (int) n: i migliori N punteggi\n",
    "    \"\"\"    \n",
    "    \n",
    "    t = PrettyTable()\n",
    "\n",
    "    print(\"Migliori parametri su insieme di validazione:\")\n",
    "    indexes = np.argsort(grid_search.cv_results_['mean_test_score'])[::-1][:n]\n",
    "    means = grid_search.cv_results_['mean_test_score'][indexes]\n",
    "    stds = grid_search.cv_results_['std_test_score'][indexes]\n",
    "    params = np.array(grid_search.cv_results_['params'])[indexes]\n",
    "    \n",
    "    t.field_names = ['Score'] + [f for f in params[0].keys()] \n",
    "    for mean, std, params in zip(means, stds, params):\n",
    "        row=[\"%0.3f (+/-%0.03f)\" % (mean, std * 2)] + [p for p in params.values()]\n",
    "        t.add_row(row)\n",
    "    print(t)\n",
    "               \n",
    "\n",
    "def param_tune_grid_cv(clf, params, X_train, y_train, cv, execution_time=False):\n",
    "    \"\"\"\n",
    "    Function that performs a grid search over some parameters\n",
    "    Funzione che effettua una gridSearch sui parametri\n",
    "\n",
    "    :param (estimator) clf: oggetto con all'interno un classificatore\n",
    "    :param (dictionary) params: dizionario di parametri su cui lavorare\n",
    "    :param (array-like) X_train: dati di allenamento\n",
    "    :param (array-like) y_train: true labels dei dati di allineamento\n",
    "    :param (cross-validation generator) cv: determina la cross-validation splitting strategy\n",
    "    \"\"\" \n",
    "    if execution_time:\n",
    "      start = time.perf_counter()\n",
    "    pipeline = Pipeline([('clf', clf)])\n",
    "    grid_search = GridSearchCV(estimator=pipeline, \n",
    "                               param_grid=params, \n",
    "                               cv=cv, \n",
    "                               n_jobs=-1,       # Use all processors\n",
    "                               scoring='f1',    # Use f1 metric for evaluation\n",
    "                               return_train_score=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    if execution_time:\n",
    "      end = time.perf_counter()\n",
    "      return grid_search, \"%.4f\" % (end-start)\n",
    "    return grid_search\n",
    "   \n",
    "\n",
    "def score(clfs, datasets):\n",
    "    \"\"\"\n",
    "    Funzione che testa un classificatore su dei dati\n",
    "    \n",
    "    :param (array of estimator) clf: vettore di classificatori\n",
    "    :param (dictionary) params: dizionario con i dati di test, passati come [(X_test, y_test)]\n",
    "\n",
    "    \"\"\"  \n",
    "    scores = []\n",
    "    for c, (X_test, y_test) in zip(clfs, datasets):\n",
    "        scores.append(c.score(X_test, y_test))\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def hexToRGBA(hex, alpha):\n",
    "\n",
    "    \"\"\"\n",
    "    Funzione che ritorna un valore rgba da valori di opacità e esadecimali\n",
    "    \n",
    "    :param (String) clf: valore esadecimale\n",
    "    :param (float) params: valore compreso tra 0 e 1 che indica livello di opacità\n",
    "\n",
    "    \"\"\"  \n",
    "\n",
    "    r = int(hex[1:3], 16)\n",
    "    g = int(hex[3:5], 16)\n",
    "    b = int(hex[5:], 16)\n",
    "\n",
    "    if alpha:\n",
    "        return \"rgba(\" + str(r) + \", \" + str(g) + \", \" + str(b) + \", \" + str(alpha) + \")\"\n",
    "    else:\n",
    "        return \"rgb(\" + str(r) + \", \" + str(g) + \", \" + str(b) + \")\"\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, cv=None, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Genera un semplice grafico di curva di apprendimento del training e del test \n",
    "    \"\"\"\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, \n",
    "                                                            X, \n",
    "                                                            y, \n",
    "                                                            cv=cv, \n",
    "                                                            n_jobs=n_jobs, \n",
    "                                                            train_sizes=train_sizes, \n",
    "                                                            scoring=\"f1\", \n",
    "                                                            random_state=RANDOM_SEED,\n",
    "                                                           )\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    # Prints lower bound (mean - std) of train \n",
    "    trace1 = go.Scatter(\n",
    "        x=train_sizes, \n",
    "        y=train_scores_mean - train_scores_std, \n",
    "        showlegend=False,\n",
    "        mode=\"lines\",\n",
    "        name=\"\",\n",
    "        hoverlabel = dict(\n",
    "            namelength=20\n",
    "        ),\n",
    "        line = dict(\n",
    "            width = 0.1,\n",
    "            color = hexToRGBA(PLOTLY_COLORS[0], 0.4),\n",
    "        ),\n",
    "    )\n",
    "    # Prints upper bound (mean + std) of train\n",
    "    trace2 = go.Scatter(\n",
    "        x=train_sizes, \n",
    "        y=train_scores_mean + train_scores_std, \n",
    "        showlegend=False,\n",
    "        fill=\"tonexty\",\n",
    "        mode=\"lines\",\n",
    "        name=\"\",\n",
    "        hoverlabel = dict(\n",
    "            namelength=20\n",
    "        ),\n",
    "        line = dict(\n",
    "            width = 0.1,\n",
    "            color = hexToRGBA(PLOTLY_COLORS[0], 0.4),\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Prints mean train score line\n",
    "    trace3 = go.Scatter(\n",
    "        x=train_sizes, \n",
    "        y=train_scores_mean, \n",
    "        showlegend=True,\n",
    "        name=\"Punteggio training\",\n",
    "        line = dict(\n",
    "            color = PLOTLY_COLORS[0],\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Prints lower bound (mean - std) of test \n",
    "    trace4 = go.Scatter(\n",
    "        x=train_sizes, \n",
    "        y=test_scores_mean - test_scores_std, \n",
    "        showlegend=False,\n",
    "        mode=\"lines\",\n",
    "        name=\"\",\n",
    "        hoverlabel = dict(\n",
    "            namelength=20\n",
    "        ),\n",
    "        line = dict(\n",
    "            width = 0.1,\n",
    "            color = hexToRGBA(PLOTLY_COLORS[1], 0.4),\n",
    "        ),\n",
    "    )\n",
    "        # Prints upper bound (mean + std) of test\n",
    "    trace5 = go.Scatter(\n",
    "        x=train_sizes, \n",
    "        y=test_scores_mean + test_scores_std, \n",
    "        showlegend=False,\n",
    "        fill=\"tonexty\",\n",
    "        mode=\"lines\",\n",
    "        name=\"\",\n",
    "        hoverlabel = dict(\n",
    "            namelength=20\n",
    "        ),\n",
    "        line = dict(\n",
    "            width = 0.1,\n",
    "            color = hexToRGBA(PLOTLY_COLORS[1], 0.4),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Prints mean test score line \n",
    "    trace6 = go.Scatter(\n",
    "        x=train_sizes, \n",
    "        y=test_scores_mean, \n",
    "        showlegend=True,\n",
    "        name=\"Punteggio test\",\n",
    "        line = dict(\n",
    "            color = PLOTLY_COLORS[1],\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    data = [trace1, trace2, trace3, trace4, trace5, trace6]\n",
    "    layout = go.Layout(\n",
    "        title=title,\n",
    "        autosize=True,\n",
    "        yaxis=dict(\n",
    "            title='Punteggio',\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=\"#Dati training \",\n",
    "        ),\n",
    "        legend=dict(\n",
    "            x=0.8,\n",
    "            y=0,\n",
    "        ),\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    return py.iplot(fig, filename=title)\n",
    "\n",
    "\n",
    "def print_confusion_matrix(gs, X_test, y_test):\n",
    "\n",
    "    \"\"\"\n",
    "    Funzione che stampa la matrice di confusione di un classificatore\n",
    "    \n",
    "    :param (estimator) clf: oggetto con all'interno un classificatore\n",
    "    :param (array-like) X_test: dati di test\n",
    "    :param (array-like) y_test: labels dei dati di test\n",
    "    \"\"\"  \n",
    "\n",
    "    gs_score = gs.score(X_test, y_test)\n",
    "    y_pred = gs.predict(X_test)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    t = PrettyTable()\n",
    "    t.add_row([\"True malati\", cm[0][0], cm[0][1]])\n",
    "    t.add_row([\"True sani\", cm[1][0], cm[1][1]])\n",
    "    t.field_names = [\" \", \"Predicted malati\", \"Predicted sani\"]\n",
    "    print(t)\n",
    "\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # normalize the confusion matrix\n",
    "    cm_df = pd.DataFrame(cm.round(3), index=[\"True Sick\", \"True Healthy\"], columns=[\"Predicted sick\", \"Predicted healthy\"])\n",
    "    cm_df\n",
    "\n",
    "\n",
    "def print_raw_score(clf, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Funzione che testa un classificatore su dei dati\n",
    "    \n",
    "    :param (array of estimator) clf: vettore di classificatori\n",
    "    :param (array-like) X_test: dati di test\n",
    "    :param (array-like) y_test: labels dei dati di test\n",
    "\n",
    "    \"\"\"  \n",
    "    print(\"Punteggio raggiunto: %0.3f\" % (score([clf], [(X_test, y_test)])[0]))\n",
    "\n",
    "\n",
    "def plot_feature_importance(feature_importance, title):\n",
    "    \"\"\"\n",
    "    Funzione che stampa l'importanza di una festure per un decision tree o un random forest classifier\n",
    "    \n",
    "    :param (dictionary) feature_importance: dizionario degli attributi più importanti ordinati\n",
    "    :param (str) title: titolo del grafico\n",
    "\n",
    "    \"\"\" \n",
    "    \n",
    "    trace1 = go.Bar(\n",
    "        x=feature_importance[:, 0],\n",
    "        y=feature_importance[:, 1],\n",
    "        marker = dict(color = PLOTLY_COLORS[0]),\n",
    "        opacity=PLOTLY_OPACITY,\n",
    "        name='Importanza feature'\n",
    "    )\n",
    "    data = [trace1]\n",
    "    layout = go.Layout(\n",
    "        title=title,\n",
    "        autosize=True,\n",
    "        margin=go.layout.Margin(l=50, r=100, b=150),\n",
    "        xaxis=dict(\n",
    "            title='feature',\n",
    "            tickangle=30\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='importanza feature',\n",
    "            automargin=True,\n",
    "        ),\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    return py.iplot(fig, filename=title)\n",
    "\n",
    "\n",
    "def print_performances(classifiers, classifier_names, auc_scores, X_test, y_test):\n",
    "  \n",
    "    \"\"\"\n",
    "     Funzione che testa un classificatore su dei dati\n",
    "    \n",
    "    :param (array of estimator) clf: vettore di classificatori\n",
    "    :param (array-like) classifier_names: nome del classificatore\n",
    "    :param (array-like) auc-score: punteggio auc\n",
    "    :param (array-like) X_test: dati di test\n",
    "    :param (array-like) y_test: labels dei dati di test \n",
    "\n",
    "    \"\"\" \n",
    "\n",
    "    accs = []\n",
    "    recalls = []\n",
    "    precision = []\n",
    "    results_table = pd.DataFrame(columns=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"auc\"])\n",
    "    for (i, clf), name, auc in zip(enumerate(classifiers), classifier_names, auc_scores):\n",
    "        y_pred = clf.predict(X_test)\n",
    "        row = []\n",
    "        row.append(accuracy_score(y_test, y_pred))\n",
    "        row.append(precision_score(y_test, y_pred))\n",
    "        row.append(recall_score(y_test, y_pred))\n",
    "        row.append(f1_score(y_test, y_pred))\n",
    "        row.append(auc)\n",
    "        row = [\"%.3f\" % r for r in row]\n",
    "        results_table.loc[name] = row\n",
    "    return results_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5, random_state=RANDOM_SEED)\n",
    "clf_nb = GaussianNB()\n",
    "clf_knn = KNeighborsClassifier()\n",
    "clf_rf = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "clf_lr = LogisticRegression(random_state=RANDOM_SEED)\n",
    "clf_svm = SVC(random_state=RANDOM_SEED)\n",
    "\n",
    "clfs = [clf_nb, clf_knn, clf_rf, clf_lr, clf_svm]\n",
    "clfs_ng = [clf_nb, clf_rf]\n",
    "TEST_PARAMS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_results = []\n",
    "all_gss = []\n",
    "times = np.zeros(2)\n",
    "t = 0\n",
    "\n",
    "for clf in clfs:\n",
    "    gs_ohc_pc, t = param_tune_grid_cv(clf, TEST_PARAMS, X_train, y_train, kf, execution_time=True)\n",
    "    times[0] = times[0] + float(t)\n",
    "\n",
    "    gs_ohc, t = param_tune_grid_cv(clf, TEST_PARAMS, X_train_pca, y_train_pca, kf, execution_time=True)\n",
    "    times[1] = times[1] + float(t)\n",
    "\n",
    "    gss = [gs_ohc_pc, gs_ohc]\n",
    "    all_gss.append(gss)\n",
    "    test_results = score(gss, [(X_test, y_test),\n",
    "                                (X_test_pca, y_test_pca)])\n",
    "    all_test_results.append(test_results)\n",
    "\n",
    "all_test_results_ng = []\n",
    "all_gss_ng = []\n",
    "times_ng = np.zeros(3)\n",
    "\n",
    "for clf in clfs_ng:\n",
    "    gs_full, t = param_tune_grid_cv(clf, TEST_PARAMS, X_train, y_train, kf, execution_time=True)\n",
    "    times_ng[0] = times_ng[0] + float(t)\n",
    "  \n",
    "    #gs_pc, t = param_tune_grid_cv(clf, TEST_PARAMS, X_train_pc, y_train_pc, kf, execution_time=True)\n",
    "    #times[1] = times[1] + float(t)\n",
    "  \n",
    "    gs_drop, t = param_tune_grid_cv(clf, TEST_PARAMS, X_train_pca, y_train_pca, kf, execution_time=True)\n",
    "    times_ng[1] = times_ng[1] + float(t)\n",
    "  \n",
    "    #gs_pc_drop, t = param_tune_grid_cv(clf, TEST_PARAMS, X_train_pc_drop, y_train_pc_drop, kf, execution_time=True)\n",
    "    #times[3] = times[3] + float(t)\n",
    "  \n",
    "    #gs_no_stalk, t = param_tune_grid_cv(clf, TEST_PARAMS, X_train, y_train, kf, execution_time=True)\n",
    "    #times_ng[2] = times_ng[2] + float(t)\n",
    "\n",
    "    gss_ng = [gs_full, gs_drop]\n",
    "    all_gss_ng.append(gss_ng)\n",
    "    test_results = score(gss_ng, [(X_test, y_test),\n",
    "                                  (X_test_pca, y_test_pca)\n",
    "                                 ])\n",
    "    all_test_results_ng.append(test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------------------------+-----------------+\n",
      "|                     | Dataset con dati normalizzati | Dataset con PCA |\n",
      "+---------------------+-------------------------------+-----------------+\n",
      "| Logistic Regression |             0.845             |      0.789      |\n",
      "|    Random Forest    |             0.833             |      0.719      |\n",
      "|         KNN         |             0.824             |      0.761      |\n",
      "|         SVM         |             0.812             |      0.761      |\n",
      "|     Naive Bayes     |             0.789             |      0.750      |\n",
      "+---------------------+-------------------------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "def print_results(column_names, row_names, values):\n",
    "    \"\"\"\n",
    "    Function that prints a table using prettytable library\n",
    "    Funzione che stampa una tabella con prettytable\n",
    "    \n",
    "    :param (array) column_names: vettore con i nomi delle colonne\n",
    "    :param (array) row_names: vettore con i nomi delle righe\n",
    "    :param (matrix) values: valori della tabella\n",
    "\n",
    "    \"\"\" \n",
    "    t = PrettyTable()\n",
    "    t.field_names = column_names\n",
    "    \n",
    "    all_rows = []\n",
    "    result_row = []\n",
    "\n",
    "    for name, results in zip(row_names, values):\n",
    "        result_row.append(name)\n",
    "        for r in results:\n",
    "            result_row.append(\"%.3f\" % r)\n",
    "        all_rows.append(result_row)\n",
    "        result_row = []\n",
    "\n",
    "    all_rows = sorted(all_rows, key=lambda kv: kv[1], reverse=True)\n",
    "    for k in all_rows:\n",
    "        t.add_row(k)\n",
    "    \n",
    "    print(t)\n",
    "\n",
    "\n",
    "dataset_strings = [\" \",\n",
    "                   \"Dataset con dati normalizzati\",\n",
    "                   \"Dataset con PCA\"]\n",
    "\n",
    "#dataset_strings_ng = [\" \", \"full dataset\",\n",
    "#                      \"dataset with dropped missing values\"]\n",
    "\n",
    "row_names = [\"Naive Bayes\", \"KNN\", \"Random Forest\", \"Logistic Regression\", \"SVM\"]\n",
    "#row_names_ng = [\"Naive Bayes\", \"Random Forest\"]\n",
    "\n",
    "print_results(dataset_strings, row_names, all_test_results)\n",
    "#print_results(dataset_strings_ng, row_names_ng, all_test_results_ng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+-------------------+\n",
      "|              | Predicted Sick | Predicted Healthy |\n",
      "+--------------+----------------+-------------------+\n",
      "|  True Sick   |       16       |         12        |\n",
      "| True Healthy |       6        |         27        |\n",
      "+--------------+----------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "print_confusion_matrix(all_gss_ng[0][np.argmin(all_test_results_ng[0])], X_test_pca, y_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-------------------------------+-----------------+\n",
      "|                             | Dataset con dati normalizzati | Dataset con PCA |\n",
      "+-----------------------------+-------------------------------+-----------------+\n",
      "|  Tempo totale training (s)  |             2.295             |      1.088      |\n",
      "| Punteggio medio del dataset |             0.820             |      0.756      |\n",
      "+-----------------------------+-------------------------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "means = np.mean(all_test_results, axis=0)\n",
    "row_names = [\"Tempo totale training (s)\", \"Punteggio medio del dataset\"]\n",
    "print_results(dataset_strings, row_names, [times, means])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il dataset che restituisce le performance medie migliori è:\n",
      "\t- Dataset con dati normalizzati, con un punteggio di 0.820\n"
     ]
    }
   ],
   "source": [
    "print(\"Il dataset che restituisce le performance medie migliori è:\")\n",
    "print(\"\\t- \" + dataset_strings[means.argmax()+1] + \", con un punteggio di \" + str(\"%.3f\" % means.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGISTIC REGRESSION\n",
    "\n",
    "Il primo classificatore che andiamo ad analizzare è la logistic regression: esso usa la funzione di Sigmoid per classificare i nostri sample.\n",
    "\n",
    "In alcuni casi, infatti, possiamo usare la regressione lineare (la quale non prova a predirre la classe ma il valore esatto partendo da un dato input **x** e calcolando un output **y**) per determinare un boundary appropriato. Tuttavia, poichè l'output è generalmente binario o discreto, esistono metodi di regressione più efficienti.\n",
    "Ricordiamo che per la classificazione siamo interessati alla probabilità condizionale $P(y | x ; \\theta$) dove $\\theta$ descrive i parametri per il nostro modello. Quando usiamo la regressione, $\\theta$ rappresenta i valori dei nostri coefficienti della regressione ($w$).\n",
    "\n",
    "* $ P(y=0 | X;\\theta) = g(w^TX) = \\frac{1}{1+e^{w^TX}} $\n",
    "* $ P(y=1 | X;\\theta) = 1 - g(w^TX) = \\frac{e^{w^TX}}{1+e^{w^TX}} $\n",
    "\n",
    "Ma come ricaviamo i parametri? In maniera simile a come funzionano gli altri problemi di regressione, cerchiamo la Maximum Likelihood Estimation per $w$. La probabilità dei dati forniti dal modello è:\n",
    "\n",
    "$$ L(y|x;w) = \\prod_{i}(1-g(x_i;w))^{y_i}\\cdot g(x_i;w)^{1-y_i} $$\n",
    "\n",
    "Usando operazioni aritmetiche come usare la funzione <code>log</code> otteniamo:\n",
    "\n",
    "$$ LL(y|x;w) = \\sum_{i=1}^{N}y_iln(1-g(x_i;w))+(1-y_i)ln(g(x_i;w)) = \\sum_{i=1}^{N}y_iln(1-g(x_i;w))-y_iln(g(x_i;w))+ln(g(x_i;w)) = $$\n",
    "\n",
    "$$ = \\sum_{i=1}^{N}y_iln\\left(\\frac{1-g(x_i;w)}{g(x_i;w)}\\right) + ln(g(x_i;w)) = \\sum_{i=1}^{N}y_iw^Tx_i - ln(1+e^{w^Tx_i}) = \\ell(w)$$\n",
    "\n",
    "$$ \\frac{\\partial \\ell(w)}{\\partial w_j} = \\sum_{i=1}^{N}x_i^j[y_i - (1 - g(x_i;w))] = \\sum_{i=1}^{N}x_i^j[y_i - P(y_i=1|x_i;w)] $$\n",
    "\n",
    "In questo modo otteniamo una funzione concava, che ci permette di arrivare a una soluzione.\n",
    "\n",
    "Questo modello, rispetto alla regressione lineare, può modellare meglio la zona compresa tra 0 e 1. Per conoscere i pesi, bisogna calcolare la MLE (Maximum Likelihood Estimation, ossia scegliere un valore che massimizzi la probabilità dei dati osservati) ed applicare l'algoritmo di gradient descent fino alla convergenza del valore di accuratezza.\n",
    "\n",
    "Essi sono:\n",
    "* **liblinear**: Solver, migliore per dataset più piccoli.\n",
    "* **C**: \"forza di regolarizzazione\", valore compreso tra 0,01 e 100. Più è piccolo il valore, maggiore è la regolarizzazione\n",
    "* **penalty**: penalità \"l1\" e \"l2\" per la regolarizzazione, che sono definite come:\n",
    "    * l1, che penalizza ogni errore allo stesso modo $ S = \\sum_{i=1}^{n}| y_i - f(x_i) | $\n",
    "    * l2, che penalizza maggiormente valori più grandi $ S = \\sum_{i=1}^{n}(y_i - f(x_i))^2 $\n",
    "\n",
    "Dove $y_i$ è la vera label mentre $f(x_i)$ è quella assegnata.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Migliori parametri su insieme di validazione:\n",
      "+------------------+--------+--------------+-------------+\n",
      "|      Score       | clf__C | clf__penalty | clf__solver |\n",
      "+------------------+--------+--------------+-------------+\n",
      "| 0.868 (+/-0.032) |  0.1   |      l2      |    lbfgs    |\n",
      "| 0.868 (+/-0.032) |  0.1   |      l2      |  newton-cg  |\n",
      "| 0.867 (+/-0.031) |  0.1   |      l2      |  liblinear  |\n",
      "| 0.865 (+/-0.014) | 0.001  |      l2      |  liblinear  |\n",
      "| 0.863 (+/-0.010) |  0.01  |      l2      |  liblinear  |\n",
      "+------------------+--------+--------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "clf_lr = LogisticRegression(random_state=RANDOM_SEED)\n",
    "gs_pc_lr = param_tune_grid_cv(clf_lr, LOGISTIC_REGRESSION_PARAMS, X_train, y_train, kf)\n",
    "print_gridcv_scores(gs_pc_lr, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"525px\"\n",
       "            src=\"https://plot.ly/~albertobenincasa/59.embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f8328431da0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_learning_curve(gs_pc_lr.best_estimator_, \"Curva di apprendimento di Logistic Regression\", \n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    cv=5,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUPPORT VECTOR MACHINE\n",
    "\n",
    "Una Support Vector Machine (SVM) è un classificatore discriminativo definito formalmente da un iperpiano di separazione.\n",
    "In altre parole, dati dei labeled sample (quindi un supervised learning), l'output dell'algoritmo genera un iperpiano ottimale che classifica poi i nuovi esempi. \n",
    "\n",
    "Tra gli algoritmi di machine learning, SVM è molto popolare per la sua potenza soprattuto nella casistica di dover risolvere un problema di classificazione binaria (come quello di questa tesina), sia in presenza di dati lineari che non lineari. \n",
    "\n",
    "Caratteristica di SVM è la presenza dell'utilizzo di funzioni kernel: esse vengono utilizzate per identificare elementi in uno spazio senza utilizzare le coordinate ma invece calcolando il prodotto interno delle immagini di tutte le coppie di dati nello spazio della funzione. Questa operazione risulta spesso computazionalmente più efficiente del calcolo delle coordinate e viene chiamata *kernel trick*. \n",
    "\n",
    "Il Support Vector Machine apprende i boundaries tra dati appartenenti a due diverse classi proiettandoli in uno spazio multidimensionale allo scopo di trovare l'iperpiano che massimizza i margini tra le due classi di dati: ma qual è l'iperpiano ottimale da scegliere, essendocene un numero infinito? \n",
    "Per calcolarlo, l'algoritmo ne genera una serie e prova a separare le istanze tramite la massimizzazione dei margini tra i due punti più vicini delle due classi.\n",
    "I dati vengono separati tramite un boundary e i punti che si trovano su di esso sono detti *vettori di supporto*: non vengono utilizzati quindi tutti i dati categorizzati ma solo quelli sui margini.\n",
    "\n",
    "Dobbiamo quindi trovare l'iperpiano che massimizza i margini. Il caso più semplice è quando i dati sono linearmente separabili. \n",
    "Pur essendo particolarmente efficace in casi di classificazione binaria, questo algoritmo si presta anche per la classificazione di classi multiple, con indici di robustezza e accuratezza inferiori. \n",
    "\n",
    "In uno spazio a due dimensioni, tale iperpiano è definito come una retta che divide il piano in due parti, ognuna che specifica una delle due classi.\n",
    "\n",
    "In questo caso abbiamo codeste specifiche:\n",
    "* **linear**: la più semplice delle SVM, trova l'iperpiano che separa nel migliore dei modi i nostri dati di training\n",
    "* **C**: il parametro C di SVM dice all'ottimizzatore una misura su quanto è importante evitare classificazioni errate. Più è alto il valore di C, più sarà piccolo il margine della retta SE essa con tale margine è in grado di classificare meglio i nostri dati.\n",
    "* **rbf**: questo parametro indica che stiamo usando una Radial Basis Function kernel per effettuare il prodotto scalare\n",
    "    * **gamma**: definisce fino a che punto il valore di un singolo elemento possa influire; se il valore di gamma è basso, significa molta influenza, con un valore alto, poca influenza. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Migliori parametri su insieme di validazione:\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Row has incorrect number of values, (actual) 3!=4 (expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-57d6a7751752>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclf_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRANDOM_SEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgs_pc_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_tune_grid_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_svm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVM_PARAMS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint_gridcv_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs_pc_svm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-58e0fda015a5>\u001b[0m in \u001b[0;36mprint_gridcv_scores\u001b[0;34m(grid_search, n)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"%0.3f (+/-%0.03f)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/prettytable.py\u001b[0m in \u001b[0;36madd_row\u001b[0;34m(self, row)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_field_names\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_field_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Row has incorrect number of values, (actual) %d!=%d (expected)\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_field_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_field_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfield_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Field %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Row has incorrect number of values, (actual) 3!=4 (expected)"
     ]
    }
   ],
   "source": [
    "clf_svm = SVC(probability=True, random_state=RANDOM_SEED)\n",
    "gs_pc_svm = param_tune_grid_cv(clf_svm, SVM_PARAMS, X_train, y_train, kf)\n",
    "print_gridcv_scores(gs_pc_svm, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~albertobenincasa/22.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_learning_curve(gs_pc_svm.best_estimator_, \"Learning curve of SVM\", \n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+-------------------+\n",
      "|              | Predicted Sick | Predicted Healthy |\n",
      "+--------------+----------------+-------------------+\n",
      "|  True Sick   |       20       |         8         |\n",
      "| True Healthy |       5        |         28        |\n",
      "+--------------+----------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "print_confusion_matrix(gs_pc_svm, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes Classifier\n",
    "\n",
    "Il Naïve Bayes Classifier è un algoritmo predittivo di tipo statistico. Uno dei fondamenti della statistica è la stima della probabilità che un evento accada oppure no. Le rilevazioni statistiche possono essere totali (prendendo, per esempio, in esame un'intera popolazione: difficili e costose) oppure campionarie. \n",
    "\n",
    "Quest'ultima modalità è chiaramente meno costosa ma porta con sè diverse problematiche. Infatti, quando si sceglie un campione, bisogna fare in modo che esso rappresenti, in piccolo, la popolazione totale: solo in questo modo possiamo trarre conclusioni che poi possono essere probabilisticamente estese.\n",
    "\n",
    "Si introduce quindi il concetto di probabilità: dato un fenomeno aleatorio, con un insieme di risultati tutti egualmente possibili, la probabilità di un evento è definita dal rapporto tra il numero di risultati favorevoli all'evento stesso e il numero di risultati possibili.\n",
    "\n",
    "Il classificatore preso in oggetto si basa sul teorema di Bayes, che afferma che $ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $ dove:\n",
    "* $P(A)$ è il prior, il grado iniziale di \"fede\" in A\n",
    "* $P(A|B)$ è il posteriore, il grado di credenza che rappresenta B\n",
    "* $P(B|A)$ è la probabilità, il grado di credenza di B, assunto che A sia vero\n",
    "\n",
    "Utilizzando il teorema di Bayes, possiamo quindi trovare la probabilità che A si verifichi, assumendo che B si è verificato.\n",
    "Ergo, in questo caso A è l'ipotesi e B la tesi.\n",
    "L'assunzione che facciamo noi è che i predittori/gli attributi siano indipendenti. Perciò la presenza di una particolare caratteristica non influenza l'altra. Per questo è chiamato \"ingenuo\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best grid scores on validation set:\n",
      "+------------------+\n",
      "|      Score       |\n",
      "+------------------+\n",
      "| 0.843 (+/-0.049) |\n",
      "+------------------+\n",
      "+--------------+----------------+-------------------+\n",
      "|              | Predicted Sick | Predicted Healthy |\n",
      "+--------------+----------------+-------------------+\n",
      "|  True Sick   |       18       |         10        |\n",
      "| True Healthy |       5        |         28        |\n",
      "+--------------+----------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "clf_nb = GaussianNB()\n",
    "gs_pc_nb = param_tune_grid_cv(clf_nb, TEST_PARAMS, X_train, y_train, kf)\n",
    "print_gridcv_scores(gs_pc_nb, n=5)\n",
    "\n",
    "print_confusion_matrix(gs_pc_nb, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~albertobenincasa/24.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_learning_curve(clf_nb, \"Learning curve of GaussianNB\", \n",
    "                    X_train, \n",
    "                    y_train, \n",
    "                    cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM FOREST CLASSIFIER\n",
    "\n",
    "E' un metodo basato sugli alberi, derivato dall'\"impacchettare\" gli alberi decisionali, al quale viene aggiunto un piccolo trucco che de-correla gli alberi, al fine di ridurre ulteriormente la varianza del dataset.\n",
    "\n",
    "Come nell'impacchettamento, costruiamo un numero di alberi decisionali sui training samples. Ma, quando li costruiamo, ogni volta che viene considerata la divisione in un albero, viene scelta una selezione casuale di *m* predittori come candidati separati dal set di *p* predittori.\n",
    "La divisione è autorizzata a selezionare solo uno di questi *m* predittori, che tipicamente sono $ m \\approx \\sqrt{p} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best grid scores on validation set:\n",
      "+------------------+----------------+----------------+-------------------+-------------------+\n",
      "|      Score       | clf__criterion | clf__max_depth | clf__max_features | clf__n_estimators |\n",
      "+------------------+----------------+----------------+-------------------+-------------------+\n",
      "| 0.853 (+/-0.027) |      gini      |       50       |        sqrt       |        100        |\n",
      "| 0.853 (+/-0.027) |      gini      |       75       |        sqrt       |        100        |\n",
      "| 0.853 (+/-0.027) |      gini      |      100       |        log2       |        100        |\n",
      "| 0.853 (+/-0.027) |      gini      |      100       |        sqrt       |        100        |\n",
      "| 0.853 (+/-0.027) |      gini      |       75       |        log2       |        100        |\n",
      "+------------------+----------------+----------------+-------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "clf_pc_rf = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "gs_pc_rf = param_tune_grid_cv(clf_pc_rf, RANDOM_FOREST_PARAMS, X_train, y_train, kf)\n",
    "print_gridcv_scores(gs_pc_rf, n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+-------------------+\n",
      "|              | Predicted Sick | Predicted Healthy |\n",
      "+--------------+----------------+-------------------+\n",
      "|  True Sick   |       20       |         8         |\n",
      "| True Healthy |       5        |         28        |\n",
      "+--------------+----------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "print_confusion_matrix(gs_pc_rf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~albertobenincasa/26.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_learning_curve(gs_pc_rf.best_estimator_, \"Learning curve of Random Forest Classifier\", \n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~albertobenincasa/28.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance = np.array(sorted(zip(X_data.columns, \n",
    "                              gs_pc_rf.best_estimator_.named_steps['clf'].feature_importances_),\n",
    "                              key=lambda x: x[1], reverse=True))\n",
    "plot_feature_importance(feature_importance, \"Feature importance in the random forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-NEAREST NEIGHBORS CLASSIFIER\n",
    "\n",
    "il KNN è un tipo di apprendimento basato sull'istanza o apprendimento pigro, in cui la funzione viene solo approssimata localmente e tutti i calcoli vengono posticipati fino alla classificazione. L'algoritmo KNN è tra i più semplici di tutti gli algoritmi di apprendimento automatico.\n",
    "\n",
    "La fase di addestramento dell'algoritmo consiste solo nel memorizzare i vettori di caratteristiche e le etichette di classe dei campioni di addestramento. Nella fase di classificazione, K è una costante definita dall'utente e un vettore senza etichetta (una query o un punto di prova) viene classificato assegnando l'etichetta più frequente tra i campioni di addestramento k più vicini a quel punto di ricerca.\n",
    "\n",
    "I parametri del cross validation sono:\n",
    "* **n_neighbors**: il numero di samples \"vicini\" da analizzare\n",
    "* **weights**: indica la funzione weight da applicare nella predizione\n",
    "    * **uniform**: tutti i punti \"nel vicinato\" sono pesati in maniera uguale\n",
    "    * **distance**: i punti sono pesati per l'inverso della loro distanza. Perciò, i punti più vicini avranno più peso di quelli più distanti\n",
    "* **p**: parametro di potenza per la metrica di Minkowski\n",
    "    * $p=1$ si usa l1\n",
    "    * $p=2$ si usa l2\n",
    "    * $p>2$ è usata la minkowsi_distance (l_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best grid scores on validation set:\n",
      "+------------------+------------------+--------+--------------+\n",
      "|      Score       | clf__n_neighbors | clf__p | clf__weights |\n",
      "+------------------+------------------+--------+--------------+\n",
      "| 0.864 (+/-0.014) |        15        |   2    |   distance   |\n",
      "| 0.864 (+/-0.063) |        5         |   2    |   uniform    |\n",
      "| 0.864 (+/-0.063) |        5         |   2    |   distance   |\n",
      "| 0.863 (+/-0.070) |        5         |   1    |   uniform    |\n",
      "| 0.860 (+/-0.066) |        5         |   1    |   distance   |\n",
      "+------------------+------------------+--------+--------------+\n"
     ]
    }
   ],
   "source": [
    "clf_knn = KNeighborsClassifier()\n",
    "gs_knn = param_tune_grid_cv(clf_knn, KNN_PARAMS, X_train, y_train, kf)\n",
    "print_gridcv_scores(gs_knn, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+-------------------+\n",
      "|              | Predicted Sick | Predicted Healthy |\n",
      "+--------------+----------------+-------------------+\n",
      "|  True Sick   |      110       |         0         |\n",
      "| True Healthy |       0        |        132        |\n",
      "+--------------+----------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "print_confusion_matrix(gs_knn, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~albertobenincasa/30.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_learning_curve(gs_knn.best_estimator_, \"Learning curve of k-NN Classifier\", \n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC CURVE\n",
    "\n",
    "Arrivati a questo punto dobbiamo comparare le performance dei diversi classificatori.\n",
    "Andiamo quindi a tracciare la ROC curve e la Area Under Curve per tutti i nostri modelli. La ROC curve è tracciata con TPR rispetto a FPR dove TPR è sull'asse y e FPR è sull'asse x. Nello specifico, questi parametri sono:\n",
    "* $ TRP/Recall/Sensitivity = \\frac{TP}{TP+FN}$\n",
    "* $FPR = \\frac{FP}{TN+FP}$\n",
    "\n",
    "Un modello eccellente ha l'AUC vicino all'1 che significa che ha una buona misura di separabilità. Un modello scadente ha l'AUC vicino allo 0, il che significa che ha la peggiore misura di separabilità."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(classifiers, legend, title, X_test, y_test):\n",
    "    t1 = go.Scatter(\n",
    "        x=[0, 1], \n",
    "        y=[0, 1], \n",
    "        showlegend=False,\n",
    "        mode=\"lines\",\n",
    "        name=\"\",\n",
    "        line = dict(\n",
    "            color = COLOR_PALETTE[0],\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    data = [t1]\n",
    "    aucs = []\n",
    "    for clf, string, c in zip(classifiers, legend, COLOR_PALETTE[1:]):\n",
    "        y_test_roc = np.array([([0, 1] if y else [1, 0]) for y in y_test])\n",
    "        y_score = clf.predict_proba(X_test)\n",
    "        \n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i in range(2):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_roc[:, i], y_score[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        # Compute micro-average ROC curve and ROC area\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_roc.ravel(), y_score.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "        aucs.append(roc_auc['micro'])\n",
    "\n",
    "        trace = go.Scatter(\n",
    "            x=fpr['micro'], \n",
    "            y=tpr['micro'], \n",
    "            showlegend=True,\n",
    "            mode=\"lines\",\n",
    "            name=string + \" (area = %0.2f)\" % roc_auc['micro'],\n",
    "            hoverlabel = dict(\n",
    "                namelength=30\n",
    "            ),\n",
    "            line = dict(\n",
    "                color = c,\n",
    "            ),\n",
    "        )\n",
    "        data.append(trace)\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title=title,\n",
    "        autosize=False,\n",
    "        width=550,\n",
    "        height=550,\n",
    "        yaxis=dict(\n",
    "            title='True Positive Rate',\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=\"False Positive Rate\",\n",
    "        ),\n",
    "        legend=dict(\n",
    "            x=0.4,\n",
    "            y=0.06,\n",
    "        ),\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    return aucs, iplot(fig, filename=title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~albertobenincasa/32.embed\" height=\"550px\" width=\"550px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers = [gs_pc_lr, gs_pc_svm, gs_pc_nb, gs_pc_rf, gs_knn]\n",
    "classifier_names = [\"Logistic Regression\", \"SVM\", \"GaussianNB\", \"Random Forest\", \"KNN\"]\n",
    "auc_scores, roc_plot = plot_roc_curve(classifiers, classifier_names, \"ROC curve\", X_test, y_test)\n",
    "roc_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora esaminiamo alcuni parametri aggiuntivi che valutano la bontà dei nostri modelli:\n",
    "1. Accuratezza\n",
    "2. Precisione \n",
    "    * $ P = \\frac{TP}{TP+FP} $\n",
    "3. Recall\n",
    "    * $ R = \\frac{TP}{TP+FN} $\n",
    "4. F1 (media ponderata della precisione e recall) \n",
    "    * $ F1 = 2 * \\frac{P*R}{P+R} $\n",
    "5. Auc, area under the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.836</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.787</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>0.754</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.787</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.820</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    accuracy precision recall     f1    auc\n",
       "Logistic Regression    0.836     0.811  0.909  0.857  0.908\n",
       "SVM                    0.787     0.778  0.848  0.812  0.880\n",
       "GaussianNB             0.754     0.737  0.848  0.789  0.874\n",
       "Random Forest          0.787     0.778  0.848  0.812  0.893\n",
       "KNN                    0.820     0.789  0.909  0.845  0.902"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_performances(classifiers, classifier_names, auc_scores, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "commenti sul grafico sopra\n",
    "\n",
    "### CONCLUSIONI\n",
    "\n",
    "Dalla letteratura medica è bene segnalare che solitamente, ad eccezione di qualche caso specifico, questo tipo di problemi diminuisce drasticamente nel caso in cui si prendano delle precauzioni quali ad esempio:\n",
    "\n",
    "* Smettere di fumare\n",
    "* Effettuare controlli periodici sulla condizione di salute di parametri come ipertensione, colesterolo alto e diabete\n",
    "* Fare esercizio almeno 30 minuti al giorno \n",
    "* Seguire una dieta sana e pover di sale e grassi saturi\n",
    "* Ridurre e gestire lo stress\n",
    "* Praticare una buona igiene\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
