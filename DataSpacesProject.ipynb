{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import di tutte le librerie che devono essere usate nel progetto\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from collections import defaultdict\n",
    "from scipy import interp\n",
    "import numpy as np\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_curve\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, auc\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "from plotly.plotly import plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import math\n",
    "from scipy.cluster import hierarchy as hc\n",
    "import scipy.spatial as scs\n",
    "import scipy.stats as ss\n",
    "import plotly.figure_factory as ff\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV, learning_curve, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, confusion_matrix, roc_curve, auc, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from collections import Counter\n",
    "from prettytable import PrettyTable\n",
    "from functools import wraps\n",
    "\n",
    "plotly.tools.set_credentials_file(username='albertobenincasa', api_key='14CCI1wSA0JGaDZhuouL')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# costanti\n",
    "PLOTLY_COLORS = ['#4A708B', '#87CEFA']\n",
    "PLOTLY_OPACITY = 0.7\n",
    "COLORSCALE_HEATMAP = [         [0.0, '#011f4b'],\n",
    "                [0.1111111111111111, '#03396c'], \n",
    "                [0.2222222222222222, '#005b96'], \n",
    "                [0.3333333333333333, '#2171b5'], \n",
    "                [0.4444444444444444, '#6497b1'], \n",
    "                [0.5555555555555556, '#6baed6'], \n",
    "                [0.6666666666666666, '#B0E2FF'], \n",
    "                [0.7777777777777778, '#b3cde0'], \n",
    "                [0.8888888888888888, '#bdd7e7'], \n",
    "                               [1.0, '#BFEFFF']] \n",
    "COLOR_PALETTE = sns.color_palette(\"Blues\").as_hex()\n",
    "RANDOM_SEED = 11\n",
    "\n",
    "LOGISTIC_REGRESSION_PARAMS = [{\n",
    "    'clf__solver': ['liblinear'],  # best for small datasets\n",
    "    'clf__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], # smaller value, stronger regularization, like svm\n",
    "    'clf__penalty': ['l2', 'l1']\n",
    "},\n",
    "{\n",
    "    'clf__solver': ['newton-cg', 'lbfgs'], \n",
    "    'clf__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'clf__penalty': ['l2'] # `newton-cg` and `lbfgs` accept only l2\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRODUZIONE\n",
    "\n",
    "Il dataset oggetto di questa analisi, visualizzabile e scaricabile all'indirizzo https://www.kaggle.com/ronitf/heart-disease-uci, consta di una serie di parametri di paziente affetti o no da disturbi di tipo cardiaco; esso deriva da un database contenente in origine 76 attributi, ma tutti le analisi effettuate si riferiscono ad un sottoinsieme di essi di 14 attributi.\n",
    "\n",
    "Scendendo più nel dettaglio, apprendiamo che questi 14 attributi specificano diverse caratteristice, di seguito elencate:\n",
    "* age : età del paziente in anni\n",
    "* sex : sesso del paziente (1=maschio, 0=femmina)\n",
    "* cp : tipologia di dolore addominale riscontrata (1=Angina pectoris tipica, 2=Angina pectoris atipica, 3=Dolore non dovuto ad Angina pectoris , 4=Asintomatico)\n",
    "* trestbps : pressione del sangue a riposo del paziente (in mmHg all'ammissione in ospedale)\n",
    "* chol : colesterolo del paziente misurato in mg/dl\n",
    "* fbs : glicemia a digiuno > 120 mg/dl (1=vero; 0=falso) \n",
    "* restecg : risutati elettrocardiografici a riposo (0=normale, 1=presenza di anormalità nell'onda ST-T, 2=probabile ipertrofia ventricolo sinistro)  \n",
    "* thalach : battito cardiaco massimo raggiunto\n",
    "* exang : angina pectoris indotta da esercizio (1=si, 0=no) \n",
    "* oldpeak : sottoslivellamento del tratto ST indotta dall'esercizio, non presente a riposo\n",
    "* slope : la pendenza durante il picco di esercizio del tratto ST\n",
    "* ca : numero di vasi maggiori (0-3) colorati per fluoroscopia\n",
    "* thal : malattia del sangue chiamata talassemia (3 = normale; 6 = difetto fisso; 7 = difetto reversibile)\n",
    "* target : presenza di malattia (1=si, 0=no)\n",
    "\n",
    "### CARICAMENTO DEL DATASET E ANALIZI PRELIMINARI\n",
    "\n",
    "Come prima cosa carichiamo il dataset in una variabile di tipo DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carico il dataset presente in locale\n",
    "dataset = pd.read_csv(\"./dataset/heart.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il passo successivo consiste nel guardare il dataset appena caricato in modo da capirne il contenuto, le diverse feature disponibili e il tipo di dato presente in quest'ultime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il dataset ha 303 record e 14 features.\n",
      "\n",
      "Ci sono 2 classi differenti:\n",
      " [1, 0]\n",
      "\n",
      "Valori unici per ogni campo: \n",
      "age          41\n",
      "sex           2\n",
      "cp            4\n",
      "trestbps     49\n",
      "chol        152\n",
      "fbs           2\n",
      "restecg       3\n",
      "thalach      91\n",
      "exang         2\n",
      "oldpeak      40\n",
      "slope         3\n",
      "ca            5\n",
      "thal          4\n",
      "target        2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Dimensione del dataset\n",
    "print(\"Il dataset ha %d record e %d features.\\n\" % dataset.shape)\n",
    "\n",
    "# Conteggio del numero di classi per la classificazione\n",
    "print(f\"Ci sono {dataset['target'].unique().size} classi differenti:\"f\"\\n {dataset['target'].unique().tolist()}\")\n",
    "\n",
    "# Conteggio del numero di valori unici per ogni colonna\n",
    "print(f\"\\nValori unici per ogni campo: \\n{dataset.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING\n",
    "\n",
    "Prima di poter passare alla classificazione, è fondamentale una fase di preprocessing dei dati. \n",
    "Questo è lo step più importante di tutti, perchè \"pulire\" i dati prima di \"mandarli in pasto\" ai diversi algoritmi di classificazione ha un duplice vantaggio: per prima cosa, evitiamo di sovraccaricare la potenza di calcolo della macchina per conti non utili alla classificazione finale, e per seconda, analizzare i dati prima di processarli ci permette di ottenere la classificazione migliore possibile.\n",
    "\n",
    "Molto utile anche controllare la presenza di eventuali valori nulli: nel caso in cui ci fossero colonne non valorizzate, è bene procedere con l'eliminazione di quest'ultime per ridurre il numero di dati su cui dobbiamo lavorare.\n",
    "\n",
    "Come possiamo notare, non vi è traccia della presenza di valori nulli nel dataset. \n",
    "La strategia alternativa all'eliminazione di intere colonne, particolarmente vantaggiosa solo se tutti i valori (o perlomeno, la maggior parte) sono non definiti è quella di inserire valori di default (come 0) per i record che non hanno valori definiti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conteggio dei valori NaN: \n",
      "age         0\n",
      "sex         0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalach     0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          0\n",
      "thal        0\n",
      "target      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Controllo la presenza di valori nulli\n",
    "print(f\"\\nConteggio dei valori NaN: \\n{dataset.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La prima, semplice, analisi da fare è quella di vedere che tipo di dati abbiamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come possiamo notare, tutti i nostri attributi sono di tipo numerico. Nonostante questo, dalla descrizione del dataset sappiamo che alcuni di questi valori corrispondono in realtà ad elementi di tipo categorico. \n",
    "\n",
    "Infatti, i campi valorizzati con delle stringhe non permettono un'analisi semplice, \n",
    "bla\n",
    "bla\n",
    "bla\n",
    "\n",
    "Vediamo ora come i nostri campioni sono suddivisi nelle diverse classi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    165\n",
      "0    138\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y = dataset['target'].value_counts()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunatamente, il dataset è abbastanza bilanciato: abbiamo circa lo stesso numero di campioni classificati come malati e non malati. Questo è molto importante perchè possiamo assegnare lo stesso peso alle due classi quando passeremo alla fase di classificazione.\n",
    "\n",
    "Inoltre, possiamo notare come il nostro compito di classificazione sarà di tipo binario: infatti, il nostro obiettivo è riuscire a predire se un paziente potrebbe riscontrare malattie cardiache oppure no. Possiamo ora approfondire alcuni dettagli statistici sul dataset utilizzando il metodo **describe()** sul nostro DataFrame di pandas. L'output mostrerà:\n",
    "* **count** -> specifica il numero dei record presenti nel dataset \n",
    "* **mean** -> specifica la media dell'attributo calcolata per tutti i record\n",
    "* **std** -> specifica la deviazione standard dell'attributo\n",
    "* **min** -> specifica il valore minimo dell'attributo\n",
    "* **25%** -> il 25% dei record ha un valore minore di quello visualizzato (lower percentile)\n",
    "* **50%** -> il 50% dei record ha un valore minore di quello visualizzato (median percentile)\n",
    "* **75%** -> il 75% dei record ha un valore minore di quello visualizzato (upper percentile)\n",
    "* **max** -> specifica il valore massimo dell'attributo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>54.366337</td>\n",
       "      <td>0.683168</td>\n",
       "      <td>0.966997</td>\n",
       "      <td>131.623762</td>\n",
       "      <td>246.264026</td>\n",
       "      <td>0.148515</td>\n",
       "      <td>0.528053</td>\n",
       "      <td>149.646865</td>\n",
       "      <td>0.326733</td>\n",
       "      <td>1.039604</td>\n",
       "      <td>1.399340</td>\n",
       "      <td>0.729373</td>\n",
       "      <td>2.313531</td>\n",
       "      <td>0.544554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.082101</td>\n",
       "      <td>0.466011</td>\n",
       "      <td>1.032052</td>\n",
       "      <td>17.538143</td>\n",
       "      <td>51.830751</td>\n",
       "      <td>0.356198</td>\n",
       "      <td>0.525860</td>\n",
       "      <td>22.905161</td>\n",
       "      <td>0.469794</td>\n",
       "      <td>1.161075</td>\n",
       "      <td>0.616226</td>\n",
       "      <td>1.022606</td>\n",
       "      <td>0.612277</td>\n",
       "      <td>0.498835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>47.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>274.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age         sex          cp    trestbps        chol         fbs  \\\n",
       "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
       "mean    54.366337    0.683168    0.966997  131.623762  246.264026    0.148515   \n",
       "std      9.082101    0.466011    1.032052   17.538143   51.830751    0.356198   \n",
       "min     29.000000    0.000000    0.000000   94.000000  126.000000    0.000000   \n",
       "25%     47.500000    0.000000    0.000000  120.000000  211.000000    0.000000   \n",
       "50%     55.000000    1.000000    1.000000  130.000000  240.000000    0.000000   \n",
       "75%     61.000000    1.000000    2.000000  140.000000  274.500000    0.000000   \n",
       "max     77.000000    1.000000    3.000000  200.000000  564.000000    1.000000   \n",
       "\n",
       "          restecg     thalach       exang     oldpeak       slope          ca  \\\n",
       "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
       "mean     0.528053  149.646865    0.326733    1.039604    1.399340    0.729373   \n",
       "std      0.525860   22.905161    0.469794    1.161075    0.616226    1.022606   \n",
       "min      0.000000   71.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000  133.500000    0.000000    0.000000    1.000000    0.000000   \n",
       "50%      1.000000  153.000000    0.000000    0.800000    1.000000    0.000000   \n",
       "75%      1.000000  166.000000    1.000000    1.600000    2.000000    1.000000   \n",
       "max      2.000000  202.000000    1.000000    6.200000    2.000000    4.000000   \n",
       "\n",
       "             thal      target  \n",
       "count  303.000000  303.000000  \n",
       "mean     2.313531    0.544554  \n",
       "std      0.612277    0.498835  \n",
       "min      0.000000    0.000000  \n",
       "25%      2.000000    0.000000  \n",
       "50%      2.000000    1.000000  \n",
       "75%      3.000000    1.000000  \n",
       "max      3.000000    1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High five! You successfully sent some data to your account on plotly. View your plot in your browser at https://plot.ly/~albertobenincasa/0 or inside your plot.ly account where it is named 'Distribuzione classi'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~albertobenincasa/0.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_bar(type, data, col, visible=False):\n",
    "    \"\"\"\n",
    "    Creazione di un istogramma\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if type == \"sick\":\n",
    "        c = PLOTLY_COLORS[0]\n",
    "    else:\n",
    "        c = PLOTLY_COLORS[1]\n",
    "    \n",
    "    return go.Histogram(\n",
    "        x = data[col],\n",
    "        name = type,\n",
    "        marker = dict(color = c),\n",
    "        visible = visible,\n",
    "        opacity = PLOTLY_OPACITY,\n",
    "    )\n",
    "\n",
    "def feature_histogram(data, feature, title):\n",
    "    \n",
    "    trace1 = create_bar(\"sick\", data[data['target'] == 1], feature, True)\n",
    "    trace2 = create_bar(\"healthy\", data[data['target'] == 0], feature, True)\n",
    "    \n",
    "    data = [trace1, trace2]\n",
    "    \n",
    "    layout = dict(\n",
    "        title=title,\n",
    "        autosize=True,\n",
    "        yaxis=dict(\n",
    "            title='value',\n",
    "            automargin=True,\n",
    "        ),\n",
    "        legend=dict(\n",
    "            x=0,\n",
    "            y=1,\n",
    "        ),\n",
    "        barmode='group',\n",
    "        bargap=0.15,\n",
    "        bargroupgap=0.1\n",
    "    )\n",
    "    fig = dict(data=data, layout=layout)\n",
    "\n",
    "    return py.iplot(fig, filename=title)\n",
    "\n",
    "feature_histogram(dataset, 'target', 'Distribuzione classi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto possiamo analizzare la distribuzione dei nostri dati usando dei boxplot: un boxplot è un modo standardizzato per visualizzare la distribuzione dei dati sulla base di un riepilogo di cinque numeri (min, lower percentile, median, upper percentile, max); risulta molto utile per visualizzare dati anomali e per controllare se i dati sono ditribuiti in modo simmetrico, quanto sono aggregati.\n",
    "\n",
    "Le informazioni che possiamo ricavare da un box plot sono:\n",
    "* **median**(Q2 / 50esimo percentile): il valore medio del dataset\n",
    "* **first quartile**(Q1 / 25esimo percentile): il valore medio tra il valore più piccolo (non il minimo) e il median del dataset\n",
    "* **third quartile**(Q3 / 75esimo percentile): il valore medio tra il valore più grande (non il massimo) e il median del dataset\n",
    "* **interquartile range**(IQR): dal 25esimo al 75esimo percentile\n",
    "* **outliers**: elementi visualizzati come singoli punti\n",
    "* **maximum**: Q3 + 1.5xIQR\n",
    "* **minimum**: Q1 - 1.5xIQR\n",
    "\n",
    "Chiaramente, ha poco senso visualizzare in questo tipo di grafici quegli attributi che sono binari o che presentano poche opzioni, per cui filtreremo questi dati prima di visualizzare i grafici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~albertobenincasa/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_box(type, data, col, visible=False):\n",
    "    \"\"\"\n",
    "    Creazione di una box\n",
    "    \"\"\"   \n",
    "    if type == 'sick':\n",
    "        c = PLOTLY_COLORS[0]\n",
    "    else:\n",
    "        c = PLOTLY_COLORS[1]\n",
    "        \n",
    "    return go.Box(\n",
    "        y = data[col],\n",
    "        name = type,\n",
    "        marker = dict(color = c),\n",
    "        visible = visible,\n",
    "        opacity = PLOTLY_OPACITY,\n",
    "    )\n",
    "\n",
    "sicks = dataset[dataset['target'] == 1]\n",
    "healthy = dataset[dataset['target'] == 0]\n",
    "box_features = [col for col in dataset.columns if ((col != 'class') and (dataset[col].nunique() > 5))]\n",
    "\n",
    "active_index = 0\n",
    "\n",
    "box_sick = [(create_box('sick', sicks, col, False) if i != active_index\n",
    "           else create_box('sick', sicks, col, True))\n",
    "             for i, col in enumerate(box_features)\n",
    "            ]\n",
    "box_healthy = [(create_box('healthy', healthy, col, False) if i != active_index\n",
    "            else create_box('healthy', healthy, col, True))\n",
    "             for i, col in enumerate(box_features)\n",
    "            ]\n",
    "\n",
    "#box_sick = []\n",
    "#box_healthy = []\n",
    "#for i, col in enumerate(box_features):\n",
    "#    if i != active_index:\n",
    "#        trace1 = create_box('sick', sicks, col, False)\n",
    "#        trace2 = create_box('healthy', healthy, col, False)\n",
    "#    else:\n",
    "#        trace1 = create_box('sick', sicks, col, True)\n",
    "#        trace2 = create_box('healthy', healthy, col, True)\n",
    "#    \n",
    "#    box_sick.append(trace1)\n",
    "#    box_healthy.append(trace2)\n",
    "\n",
    "data = box_sick + box_healthy\n",
    "number_of_features = len(box_features)\n",
    "steps = []\n",
    "\n",
    "for i in range(number_of_features):\n",
    "    step = dict(\n",
    "        method = 'restyle',  \n",
    "        args = ['visible', [False] * number_of_features],\n",
    "        label = box_features[i],\n",
    "    )\n",
    "    step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n",
    "    steps.append(step)\n",
    "    \n",
    "sliders = [dict(\n",
    "    active = active_index,\n",
    "    currentvalue = dict(\n",
    "        prefix = \"Feature: \", \n",
    "        xanchor= 'center',\n",
    "    ),\n",
    "    pad = {\"t\": 50},\n",
    "    steps = steps,\n",
    "    len=1,\n",
    ")]\n",
    "\n",
    "layout = dict(\n",
    "    sliders=sliders,\n",
    "    autosize=True,\n",
    "    yaxis=dict(\n",
    "        title='valori',\n",
    "        automargin=True,\n",
    "    ),\n",
    "    legend=dict(\n",
    "        x=0,\n",
    "        y=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "py.iplot(fig, filename='box_slider')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "commenti sul grafico sopra\n",
    "\n",
    "introduzione agli istrogrammi di sotto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~albertobenincasa/4.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_features = [col for col in dataset.columns if (col != 'target')]\n",
    "\n",
    "active_index = 0\n",
    "hist_sick = [(create_bar('sick', sicks, col, False) if i != active_index\n",
    "           else create_bar('sick', sicks, col, True))\n",
    "             for i, col in enumerate(hist_features)\n",
    "            ]\n",
    "hist_healthy = [(create_bar('healthy', healthy, col, False) if i != active_index\n",
    "           else create_bar('healthy', healthy, col, True))\n",
    "             for i, col in enumerate(hist_features)\n",
    "            ]\n",
    "\n",
    "total_data = hist_sick + hist_healthy\n",
    "number_of_features = len(hist_features)\n",
    "steps = []\n",
    "\n",
    "for i in range(number_of_features):\n",
    "    step = dict(\n",
    "        method = 'restyle',\n",
    "        args = ['visible', [False] * number_of_features],\n",
    "        label = hist_features[i],\n",
    "    )\n",
    "    step['args'][1][i] = True\n",
    "    steps.append(step)\n",
    "    \n",
    "sliders = [dict(\n",
    "    active = active_index,\n",
    "    currentvalue = dict(\n",
    "        prefix = \"Feature: \", \n",
    "        xanchor= 'center',\n",
    "    ),\n",
    "    pad = {\"t\": 50},\n",
    "    steps = steps,\n",
    "    len=1,\n",
    ")]\n",
    "\n",
    "layout = dict(\n",
    "    sliders=sliders,\n",
    "    autosize=True,\n",
    "    yaxis=dict(\n",
    "        title='valori',\n",
    "        automargin=True,\n",
    "    ),\n",
    "    legend=dict(\n",
    "        x=0,\n",
    "        y=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig = dict(data=total_data, layout=layout)\n",
    "py.iplot(fig, filename='bar_slider')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come possiamo notare dal grafico soprastante, valori come *age*, *trestbs* e *chol* non sono troppo determinanti per la classificazione. Altri attributi come *cp*, *thal* e *slope* hanno valori tutto sommato ben distinti tra le due classi: questi saranno gli attributi che saranno più importanti nella fase di classificazione.\n",
    "\n",
    "Una matrice di correlazione è una tabella che mostra i coefficienti di correlazione tra insiemi di variabili. \n",
    "Ogni variabile casuale ($X_i$) nella tabella è correlata con ciascuno degli altri valori nella tabella ($X_j$); questo permette di vedere quali coppie hanno la più alta correlazione.\n",
    "La correlazione si riferisce a qualsiasi associazione statistica, ma nell'uso comune del termine si indica quanto due variabili siano vicine ad avere una relazione lineare l'una con l'altra.\n",
    "\n",
    "In questo frangente useremo la correlazione di Pearson, che è una misura della correlazione lineare tra due variabili $X$ e $Y$. Secondo la diseguaglianza di Cauchy-Schwartz, tale valore è compreso tra +1 e -1, dove +1 è la correlazione positiva totale, 0 è nessuna correlazione lineare e -1 è una correlazione negativa totale.\n",
    "\n",
    "Questo coefficiente è calcolato come $ \\frac{cov(X, Y)}{\\sigma_X\\sigma_Y} $ dove:\n",
    "* $cov$ si chiama covarianza ed è calcolata come ($\\mu$ è il valore medio)  $$ cov(X, Y) = E[(X - \\mu_X)(Y - \\mu_Y)] $$\n",
    "* $\\sigma_X$ è la deviazione standard di $X$ $$ \\sigma_X = \\sqrt{E[X^2] - (E[X])^2} $$\n",
    "* $\\sigma_Y$ è la deviazione standard di $Y$ $$ \\sigma_Y = \\sqrt{E[Y^2] - (E[Y])^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(matrix, title):\n",
    "\n",
    "    z_text = np.around(matrix.values.tolist(), decimals=2)\n",
    "\n",
    "    figure = ff.create_annotated_heatmap(z=matrix.values, \n",
    "                                         x=matrix.columns.tolist(), \n",
    "                                         y=matrix.index.tolist(),\n",
    "                                         annotation_text=z_text,\n",
    "                                         colorscale=COLORSCALE_HEATMAP,\n",
    "                                         showscale=True)\n",
    "\n",
    "    figure.layout.title = title\n",
    "    figure.layout.autosize = False\n",
    "    figure.layout.width = 850\n",
    "    figure.layout.height = 850\n",
    "    figure.layout.margin = go.layout.Margin(l=140, r=100, b=200, t=80)\n",
    "    figure.layout.xaxis.update(side='bottom')\n",
    "    figure.layout.yaxis.update(side='left')\n",
    "\n",
    "    for i in range(len(figure.layout.annotations)):\n",
    "        figure.layout.annotations[i].font.size = 8\n",
    "                                    \n",
    "    return py.iplot(figure, filename=title)\n",
    "\n",
    "def plot_correlation_row(matrix, title):\n",
    "    \n",
    "    matrix = pd.Series.to_frame(matrix.loc['class']).transpose()\n",
    "    z_text = np.around(matrix.values.tolist(), decimals=2)\n",
    "\n",
    "    figure = ff.create_annotated_heatmap(z=matrix.values, \n",
    "                                         x=matrix.columns.tolist(), \n",
    "                                         y=matrix.index.tolist(),\n",
    "                                         annotation_text=z_text,\n",
    "                                         colorscale=COLORSCALE_HEATMAP,\n",
    "                                         showscale=False)\n",
    "\n",
    "    figure.layout.title = title\n",
    "    figure.layout.autosize = False\n",
    "    figure.layout.width = 850\n",
    "    figure.layout.height = 220\n",
    "    figure.layout.xaxis.update(side='bottom')\n",
    "    figure.layout.yaxis.update(side='left')\n",
    "\n",
    "    for i in range(len(figure.layout.annotations)):\n",
    "        figure.layout.annotations[i].font.size = 8\n",
    "\n",
    "\n",
    "    return py.iplot(figure, filename=title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~albertobenincasa/6.embed\" height=\"850px\" width=\"850px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_dataset = dataset.corr(method='pearson')\n",
    "plot_correlation_matrix(corr_dataset, 'Matrice di correlazione')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ma un momento, questo tipo di grafico ci da tutte le informazioni che effettivamente ci servono? In realtà alcune variabili numeriche definiscono dei valori che sono di tipo categorico, per cui utilizzare un'analisi geometrica come quella di Pearson ha poco senso. \n",
    "\n",
    "Per arrivare ad ottenere un risultato più consistente, abbiamo quindi bisogno di qualcosa che sia simile alla correlazione, ma che funzioni anche per le varibili di tipo categorico: necessitiamo di una *misura di associazione* che non sia però simmetrica, perchè in caso contrario corriamo il rischio di perdere informazioni come ad esempio *l'attributo x implica la label y ma la label y non implica la feature x*.\n",
    "\n",
    "**Theil's U**, anche conosciuta come **Uncertanty Coefficient** si adatta perfettamente alla nostra situazione. E' basato sull'entropia condizionale tra le variabili x e y; in altre parole, dato un valore X, esso mostra quanti stati ha y, e quanto spesso essi occorrono. L'input è del tipo $[0, 1]$ ed è asimmetrico (ossia $U(x, y) \\neq U(y, x)$ ).\n",
    "\n",
    "Il coefficiente è definito come: \n",
    "\n",
    "$$ U(X|Y) = \\frac{H(X) - H(X|Y)}{H(X)} $$\n",
    "\n",
    "dove:\n",
    "* $ H(X) = \\sum_x P_X(x)log(P_X(x)) $\n",
    "* $ H(X|Y) = - \\sum_{x,y} P_{X,Y}(x,y)log(P_{X,Y}(x,y)) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(x, y):\n",
    "    # entropy of x given y\n",
    "    y_counter = Counter(y)\n",
    "    xy_counter = Counter(list(zip(x,y)))\n",
    "    total_occurrences = sum(y_counter.values())\n",
    "    entropy = 0\n",
    "    for xy in xy_counter.keys():\n",
    "        p_xy = xy_counter[xy] / total_occurrences\n",
    "        p_y = y_counter[xy[1]] / total_occurrences\n",
    "        entropy += p_xy * math.log(p_y/p_xy)\n",
    "    return entropy\n",
    "\n",
    "def theil_u(x, y):\n",
    "    s_xy = conditional_entropy(x, y)\n",
    "    x_counter = Counter(x)\n",
    "    total_occurrences = sum(x_counter.values())\n",
    "    p_x = list(map(lambda n: n/total_occurrences, x_counter.values()))\n",
    "    s_x = ss.entropy(p_x)\n",
    "    if s_x == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return (s_x - s_xy) / s_x\n",
    "\n",
    "def create_theil_matrix(data):\n",
    "\n",
    "    columns = data.columns\n",
    "    matrix = []\n",
    "    for col in columns:\n",
    "        row = []\n",
    "        for j in range(0, len(columns)):\n",
    "            u = np.around(theil_u(data[col].tolist(), data[columns[j]].tolist()), decimals=4)\n",
    "            row.append(u)\n",
    "        matrix.append(row)\n",
    "    \n",
    "    matrix = np.array(matrix)\n",
    "\n",
    "    matrix = pd.DataFrame(data=matrix,\n",
    "                 columns=columns,\n",
    "                 index=columns)\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~albertobenincasa/8.embed\" height=\"850px\" width=\"850px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_dataset = create_theil_matrix(dataset)\n",
    "plot_correlation_matrix(corr_dataset, \"Theil_u correlation matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~albertobenincasa/10.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_histogram(dataset, 'chol', 'distribuzione chol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~albertobenincasa/12.embed\" height=\"600px\" width=\"800px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = dataset.columns\n",
    "inverse_correlation = 1 - corr_dataset # This is the 'dissimilarity' method\n",
    "\n",
    "fig = ff.create_dendrogram(inverse_correlation, \n",
    "                           labels=names, \n",
    "                           colorscale=COLOR_PALETTE, \n",
    "                           linkagefun=lambda x: hc.linkage(x, 'average'))\n",
    "\n",
    "fig['layout'].update(dict(\n",
    "    title=\"Dendogramma di correlazione tra gli attributi\",\n",
    "    width=800, \n",
    "    height=600,\n",
    "    xaxis=dict(\n",
    "        title='Features',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Distance',\n",
    "    ),\n",
    "))\n",
    "iplot(fig, filename='dendrogram_corr_clustering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NORMALIZZAZIONE DEI DATI\n",
    "\n",
    "La maggior parte delle volte, i set di dati contengono caratteristiche molto variabili in termini di grandezze, unità di misura e range. \n",
    "Poichè la maggior parte degli algoritmi di machine learning utilizza la distanza euclidea per misurare la distanza tra due dati nei loro calcoli, questo potrebbe causare dei problemi.\n",
    "\n",
    "Se lasciati così come sono, questi algoritmi prendono solo la grandezza delle funzioni che trascurano le unità: i risultati variano molto tra le diverse unità, ad esempio tra 5Kg e 5000g.\n",
    "\n",
    "Per questo, le caratteristiche con grandezze elevate peseranno molto di più nei calcoli rispetto agli attributi con minor grandezza. Per risolvere questo problema, dobbiamo portare tutti gli attributi sullo stesso livello di grandezza e ciò si può fare con il ridimensionamento.\n",
    "\n",
    "Per fare ciò, useremo lo StandardScaler, che standardizza i nostri dati sia con la media che con la deviazione standard. L'operazione matematica eseguita sarà: $$ x' = \\frac{x - \\mu_x}{\\sigma_x} $$\n",
    "Dopodichè, spezzeremo il nostro dataset in un array di dati non classificati e un array di label, che utilizzeremo nella fase di classificazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dataframe_to_array(data):\n",
    "    X_data = data.drop(['target'], axis=1)\n",
    "    y_data = data['target']\n",
    "    return X_data, y_data\n",
    "\n",
    "def scale_data(X_data):\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True, copy=True)\n",
    "    return scaler.fit_transform(X_data)\n",
    "\n",
    "X_data, y_data = dataframe_to_array(dataset)\n",
    "X_scaled_data = scale_data(X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRINCIPAL COMPONENT ANALYSIS\n",
    "\n",
    "Quando i nostri dati sono rappresentati da una matrice troppo grande (ossia il numero di dimensioni è troppo alto) risulta difficile estrarre le caratteristiche più interessanti e trovare correlazioni tra di loro, senza considerare che lo spazio occupato è più alto del necessario.\n",
    "La PCA è una tecnica che consente di ridurre la dimensionalità dei dati preservando le differenze più importanti che interccorrono tra i campioni.\n",
    "\n",
    "Questa trasformazione è definiti in modo tale che la prima componente principale abbia la varianza più grande possibile (ovvero, tiene conto il più possibile della variabilità dei valori assunti da una variabile) e ciascun componente successivo ha a sua volta la varianza più alta possibile sotto il vincolo di essere ortogonale alle componenti precedenti.\n",
    "I vettori risultanti (essendo ciascuno una combinazione lineare delle variabili e contenente N osservazioni) costituiscono una base ortogonale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_variance(pca, title):\n",
    "    \n",
    "    \"\"\"\n",
    "    Grafico della varianza cumulativa di tutte le PC\n",
    "    \"\"\"   \n",
    "\n",
    "    tot_var = np.sum(pca.explained_variance_)\n",
    "    ex_var = [(i / tot_var) * 100 for i in sorted(pca.explained_variance_, reverse=True)]\n",
    "    cum_ex_var = np.cumsum(ex_var)\n",
    "\n",
    "    cum_var_bar = go.Bar(\n",
    "        x=list(range(1, len(cum_ex_var) + 1)), \n",
    "        y=ex_var,\n",
    "        name=\"Varianza di ogni componente\",\n",
    "        marker=dict(\n",
    "            color=PLOTLY_COLORS[0],\n",
    "        ),\n",
    "        opacity=PLOTLY_OPACITY\n",
    "        )\n",
    "\n",
    "    variance_line = go.Scatter(\n",
    "        x=list(range(1, len(cum_ex_var) + 1)),\n",
    "        y=cum_ex_var,\n",
    "        mode='lines+markers',\n",
    "        name=\"Varianza cumulativa\",\n",
    "        marker=dict(\n",
    "            color=PLOTLY_COLORS[1],\n",
    "        ),\n",
    "        opacity=PLOTLY_OPACITY,\n",
    "        line=dict(\n",
    "            shape='hv',\n",
    "        ))\n",
    "    data = [cum_var_bar, variance_line]\n",
    "    layout = go.Layout(\n",
    "        autosize=True,\n",
    "        title=title,\n",
    "        yaxis=dict(\n",
    "            title='Varianza (%)',\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=\"Componenti principali\",\n",
    "            dtick=1,\n",
    "            rangemode='nonnegative'\n",
    "        ),\n",
    "        legend=dict(\n",
    "            x=0,\n",
    "            y=1,\n",
    "        ),\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    return iplot(fig, filename=title)\n",
    "\n",
    "def compress_data(X_dataset, n_components, plot_comp=False, graph_title=''):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs pca reduction of a dataset.\n",
    "\n",
    "    :param (array of arrays) X_dataset: Dataset to reduce\n",
    "    :param (int) n_components: N components to project on \n",
    "    :param (bool) plot_comp: Plot explained variance\n",
    "\n",
    "    :returns (pandas dataframe) X_df_reduced: pandas dataframe with reduced dataset\n",
    "    :return (iplot) p: Plot, only if plot_com equals True. To plot the graph,\n",
    "                       simply call the return value.\n",
    "    \"\"\"   \n",
    "\n",
    "    pca = PCA(random_state=11) #random seed\n",
    "    projected_data = pca.fit_transform(X_dataset)\n",
    "\n",
    "    if plot_comp:\n",
    "        p = plot_cumulative_variance(pca, graph_title)\n",
    "\n",
    "    pca.components_ = pca.components_[:n_components]\n",
    "    reduced_data = np.dot(projected_data, pca.components_.T)\n",
    "    X_df_reduced = pd.DataFrame(reduced_data, columns=[\"PC#%d\" % (x + 1) for x in range(n_components)])\n",
    "    if plot_comp:\n",
    "        return p, X_df_reduced\n",
    "    else:\n",
    "        return X_df_reduced\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~albertobenincasa/18.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot, X_df_ohc_reduced = compress_data(X_dataset=X_scaled_data,\n",
    "              n_components=13,\n",
    "              plot_comp=True,\n",
    "              graph_title=\"Varianza individuale e cumulativa\")\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC#1</th>\n",
       "      <th>PC#2</th>\n",
       "      <th>PC#3</th>\n",
       "      <th>PC#4</th>\n",
       "      <th>PC#5</th>\n",
       "      <th>PC#6</th>\n",
       "      <th>PC#7</th>\n",
       "      <th>PC#8</th>\n",
       "      <th>PC#9</th>\n",
       "      <th>PC#10</th>\n",
       "      <th>PC#11</th>\n",
       "      <th>PC#12</th>\n",
       "      <th>PC#13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.674659</td>\n",
       "      <td>2.095604</td>\n",
       "      <td>3.809485</td>\n",
       "      <td>0.111267</td>\n",
       "      <td>1.151868</td>\n",
       "      <td>-1.089498</td>\n",
       "      <td>0.224776</td>\n",
       "      <td>-0.555006</td>\n",
       "      <td>-0.364659</td>\n",
       "      <td>1.480557</td>\n",
       "      <td>-0.116801</td>\n",
       "      <td>-0.193078</td>\n",
       "      <td>0.089707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.732016</td>\n",
       "      <td>1.850302</td>\n",
       "      <td>-0.558395</td>\n",
       "      <td>-0.096317</td>\n",
       "      <td>2.620567</td>\n",
       "      <td>-1.278350</td>\n",
       "      <td>0.989618</td>\n",
       "      <td>-0.101235</td>\n",
       "      <td>1.425391</td>\n",
       "      <td>1.339730</td>\n",
       "      <td>1.428196</td>\n",
       "      <td>0.094293</td>\n",
       "      <td>0.304661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.523563</td>\n",
       "      <td>-0.160394</td>\n",
       "      <td>-0.290459</td>\n",
       "      <td>-0.790662</td>\n",
       "      <td>-0.092465</td>\n",
       "      <td>1.432148</td>\n",
       "      <td>1.631711</td>\n",
       "      <td>0.391299</td>\n",
       "      <td>0.744643</td>\n",
       "      <td>1.034625</td>\n",
       "      <td>0.638197</td>\n",
       "      <td>-1.221829</td>\n",
       "      <td>0.216461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.181329</td>\n",
       "      <td>-0.676779</td>\n",
       "      <td>0.040449</td>\n",
       "      <td>0.579549</td>\n",
       "      <td>0.353748</td>\n",
       "      <td>-0.006323</td>\n",
       "      <td>-0.480461</td>\n",
       "      <td>0.870860</td>\n",
       "      <td>1.521938</td>\n",
       "      <td>-0.195859</td>\n",
       "      <td>0.614323</td>\n",
       "      <td>-1.021186</td>\n",
       "      <td>-0.185181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PC#1      PC#2      PC#3      PC#4      PC#5      PC#6      PC#7  \\\n",
       "0  0.674659  2.095604  3.809485  0.111267  1.151868 -1.089498  0.224776   \n",
       "1  0.732016  1.850302 -0.558395 -0.096317  2.620567 -1.278350  0.989618   \n",
       "2 -0.523563 -0.160394 -0.290459 -0.790662 -0.092465  1.432148  1.631711   \n",
       "3  0.181329 -0.676779  0.040449  0.579549  0.353748 -0.006323 -0.480461   \n",
       "\n",
       "       PC#8      PC#9     PC#10     PC#11     PC#12     PC#13  \n",
       "0 -0.555006 -0.364659  1.480557 -0.116801 -0.193078  0.089707  \n",
       "1 -0.101235  1.425391  1.339730  1.428196  0.094293  0.304661  \n",
       "2  0.391299  0.744643  1.034625  0.638197 -1.221829  0.216461  \n",
       "3  0.870860  1.521938 -0.195859  0.614323 -1.021186 -0.185181  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df_ohc_reduced.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cercheremo ora di utilizzare uno scatter-plot per mostrare se un algoritmo di clustering applicato sulle prime due componenti principali è in grado di separare i campioni in due cluster diversi.\n",
    "\n",
    "PCA cerca di trovare combinazioni di features che conducono alla massima separazione tra i dati; ciò significa che, se avessimo una dimensione nel nostro dataset che rimane la stessa per tutti i dati, questa non verrebbe considerata, da solo o come combinazione, tra le componenti principali. Solo le caratteristiche che variano molto da dato a dato fanno parte di esse. Di conseguenza, i punti dovrebbero apparire abbastanza distanti l'uno dall'altro sul grafico.\n",
    "\n",
    "Il grafico potrebbe avere anche poco senso, e ciò può avvenire per due fattori principali \n",
    "1. E' presente molta varianza nel dataset, e perciò le prime due componenti non bastano a rappresentare significativamente i dati\n",
    "2. L'algoritmo di clustering si focalizza su features che non sono considerate importanti da PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~albertobenincasa/16.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = X_scaled_data\n",
    "pca = PCA(n_components=2)\n",
    "x = pca.fit_transform(values)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=11)\n",
    "X_clustered = kmeans.fit_predict(values)\n",
    "\n",
    "c1_idx = np.where(X_clustered == 0)\n",
    "c2_idx = np.where(X_clustered == 1)\n",
    "\n",
    "p1 = go.Scatter(\n",
    "    x=np.take(x[:,0], indices=c1_idx)[0],\n",
    "    y=np.take(x[:,1], indices=c1_idx)[0],\n",
    "    mode='markers',\n",
    "    name=\"Cluster1\",\n",
    "    marker=dict(\n",
    "        color=COLOR_PALETTE[2],\n",
    "    ),\n",
    "    opacity=PLOTLY_OPACITY)\n",
    "\n",
    "p2 = go.Scatter(\n",
    "    x=np.take(x[:,0], indices=c2_idx)[0],\n",
    "    y=np.take(x[:,1], indices=c2_idx)[0],\n",
    "    mode='markers',\n",
    "    name=\"Cluster2\",\n",
    "    marker=dict(\n",
    "        color=COLOR_PALETTE[5],\n",
    "    ),\n",
    "    opacity=PLOTLY_OPACITY)\n",
    "\n",
    "data = [p1, p2]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Dati clusterizzati usando le prime due componenti',\n",
    "    autosize=True,\n",
    "    yaxis=dict(\n",
    "        title='Seconda componente',\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Prima componente\",\n",
    "        dtick=1,\n",
    "    ),\n",
    "    legend=dict(\n",
    "        x=0,\n",
    "        y=1,\n",
    "    ),\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig, filename='clusters-scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATI NON CATEGORICI\n",
    "\n",
    "Partiamo con l'analizzare i dati di tipo non categoriche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f,ax=plt.subplots(3,2,figsize=(12,12))\n",
    "#f.delaxes(ax[2,1])\n",
    "#\n",
    "#numerical_features = ['age','thalach','chol','trestbps','oldpeak']\n",
    "#\n",
    "#for i,feature in enumerate(numerical_features):\n",
    "#    sns.distplot(dataset[feature], ax=ax[i//2,i%2], kde_kws={\"color\":\"white\"}, hist=False )\n",
    "#\n",
    "#    # Get the two lines from the ax[i//2,i%2]es to generate shading\n",
    "#    l1 = ax[i//2,i%2].lines[0]\n",
    "#\n",
    "#    # Get the xy data from the lines so that we can shade\n",
    "#    x1 = l1.get_xydata()[:,0]\n",
    "#    y1 = l1.get_xydata()[:,1]\n",
    "#    ax[i//2,i%2].fill_between(x1,y1, color=\"#4791C5\", alpha=0.8)\n",
    "#\n",
    "#    #grid\n",
    "#    #ax[i//2,i%2].grid(b=True, which='major', color='grey', linewidth=0.3)\n",
    "#    \n",
    "#    ax[i//2,i%2].set_title('Distribution of {}'.format(feature), fontsize=18)\n",
    "#    ax[i//2,i%2].set_ylabel('count', fontsize=12)\n",
    "#    ax[i//2,i%2].set_xlabel('Modality', fontsize=12)\n",
    "#\n",
    "#    #sns.despine(ax[i//2,i%2]=ax[i//2,i%2], left=True)\n",
    "#    ax[i//2,i%2].set_ylabel(\"frequency\", fontsize=12)\n",
    "#    ax[i//2,i%2].set_xlabel(str(feature), fontsize=12)\n",
    "#    \n",
    "#plt.tight_layout()\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical_features = ['age','thalach','chol','trestbps','oldpeak']\n",
    "##hist_features = [col for col in dataset.columns if col in numerical_features]\n",
    "#\n",
    "#steps = []\n",
    "#active_index = 0\n",
    "#\n",
    "#number_of_features = len(numerical_features)\n",
    "#\n",
    "#data = [(create_distplot(numerical_features[i], dataset[numerical_features[i]], col, False) if i!=active_index\n",
    "#         else create_distplot(numerical_features[i], dataset[numerical_features[i]], col, True))\n",
    "#        for i, col in zip(range(number_of_features), enumerate(numerical_features))]\n",
    "#\n",
    "#for i in range(number_of_features):\n",
    "#    step = dict(\n",
    "#        method = 'restyle',\n",
    "#        args = ['visible', [False] * number_of_features],\n",
    "#        label = numerical_features[i],\n",
    "#    )\n",
    "#    step['args'][1][i] = True\n",
    "#    steps.append(step)\n",
    "#    \n",
    "#sliders = [dict(\n",
    "#    active = active_index,\n",
    "#    currentvalue = dict(\n",
    "#        prefix = 'Feature: ',\n",
    "#        xanchor = 'center',\n",
    "#    ),\n",
    "#    pad = {\"t\": 50},\n",
    "#    steps = steps,\n",
    "#    len = 1,\n",
    "#)]\n",
    "#    \n",
    "#layout = dict(\n",
    "#    sliders = sliders,\n",
    "#    autosize = True,\n",
    "#    yaxis = dict(\n",
    "#        title = 'Frequency',\n",
    "#        automargin = True,\n",
    "#    ),\n",
    "#    barmode = 'group',\n",
    "#    bargap = 0.15,\n",
    "#    bargroupgap = 0.1\n",
    "#)\n",
    "#    \n",
    "#fig = dict(data=data, layout=layout)\n",
    "#py.iplot(fig, filename='numerical_slider')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATI CATEGORICI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f,ax=plt.subplots(4,2,figsize=(12,12))\n",
    "#\n",
    "#for i,feature in enumerate(['sex','cp','fbs','restecg','exang','slope','ca','thal']):\n",
    "#    sns.countplot(x=feature,data=dataset,ax=ax[i//2,i%2], palette = 'Blues_d', alpha=0.8, edgecolor=('white'), linewidth=2)\n",
    "#    ax[i//2,i%2].set_title('Count of {}'.format(feature), fontsize=18)\n",
    "#    ax[i//2,i%2].set_ylabel('count', fontsize=12)\n",
    "#    ax[i//2,i%2].set_xlabel('modality', fontsize=12)\n",
    "#\n",
    "#\n",
    "#plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASSIFICAZIONE\n",
    "\n",
    "In questa fase esploreremo diversi metodi di supervised learning e vedremo alla fine quale di questi classifica meglio i nostri dati.\n",
    "\n",
    "Prima di iniziare, vediamo quale tipo di pre-processed data è meglio utilizzare. Siccome il nostro dataset è particolarmente piccolo, sicuramente la riduzione della dimensionalità vista poco sopra con PCA non è necessaria.\n",
    "\n",
    "La prima cosa da fare è splittare i dati:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled_data,y_data, test_size=0.2, random_state=RANDOM_SEED, stratify=y_data)\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_df_ohc_reduced,y_data, test_size=0.2, random_state=RANDOM_SEED, stratify=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gridcv_scores(grid_search, n=5):\n",
    "    \"\"\"\n",
    "    Prints the best score achieved by a grid_search, alongside with its parametes\n",
    "\n",
    "    :param (estimator) clf: Classifier object\n",
    "    :param (int) n: Best n scores \n",
    "    \"\"\"    \n",
    "    \n",
    "    t = PrettyTable()\n",
    "\n",
    "    print(\"Best grid scores on validation set:\")\n",
    "    indexes = np.argsort(grid_search.cv_results_['mean_test_score'])[::-1][:n]\n",
    "    means = grid_search.cv_results_['mean_test_score'][indexes]\n",
    "    stds = grid_search.cv_results_['std_test_score'][indexes]\n",
    "    params = np.array(grid_search.cv_results_['params'])[indexes]\n",
    "    \n",
    "    t.field_names = ['Score'] + [f for f in params[0].keys()] \n",
    "    for mean, std, params in zip(means, stds, params):\n",
    "        row=[\"%0.3f (+/-%0.03f)\" % (mean, std * 2)] + [p for p in params.values()]\n",
    "        t.add_row(row)\n",
    "    print(t)\n",
    "               \n",
    "\n",
    "def param_tune_grid_cv(clf, params, X_train, y_train, cv, execution_time=False):\n",
    "    \"\"\"\n",
    "    Function that performs a grid search over some parameters\n",
    "\n",
    "    :param (estimator) clf: Classifier object\n",
    "    :param (dictionary) params: parameters to be tested in grid search\n",
    "    :param (array-like) X_train: List of data to be trained with\n",
    "    :param (array-like) y_train: Target relative to X for classification or regression\n",
    "    :param (cross-validation generator) cv: Determines the cross-validation splitting strategy\n",
    "    \"\"\" \n",
    "    if execution_time:\n",
    "      start = time.perf_counter()\n",
    "    pipeline = Pipeline([('clf', clf)])\n",
    "    grid_search = GridSearchCV(estimator=pipeline, \n",
    "                               param_grid=params, \n",
    "                               cv=cv, \n",
    "                               n_jobs=-1,       # Use all processors\n",
    "                               scoring='f1',    # Use f1 metric for evaluation\n",
    "                               return_train_score=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    if execution_time:\n",
    "      end = time.perf_counter()\n",
    "      return grid_search, \"%.4f\" % (end-start)\n",
    "    return grid_search\n",
    "   \n",
    "\n",
    "def score(clfs, datasets):\n",
    "    \"\"\"\n",
    "    Function that scores a classifier on some data\n",
    "    \n",
    "    :param (array of estimator) clf: Array of classifiers\n",
    "    :param (dictionary) params: Dictionary of test data, passed like [(X_test, y_test)]\n",
    "\n",
    "    \"\"\"  \n",
    "    scores = []\n",
    "    for c, (X_test, y_test) in zip(clfs, datasets):\n",
    "        scores.append(c.score(X_test, y_test))\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def hexToRGBA(hex, alpha):\n",
    "\n",
    "    \"\"\"\n",
    "    Function that returns an rgba value from an hex and an opacity value\n",
    "    \n",
    "    :param (String) clf: Hex value \n",
    "    :param (float) params: Value between 0 and 1 indicating opacity\n",
    "\n",
    "    \"\"\"  \n",
    "\n",
    "    r = int(hex[1:3], 16)\n",
    "    g = int(hex[3:5], 16)\n",
    "    b = int(hex[5:], 16)\n",
    "\n",
    "    if alpha:\n",
    "        return \"rgba(\" + str(r) + \", \" + str(g) + \", \" + str(b) + \", \" + str(alpha) + \")\"\n",
    "    else:\n",
    "        return \"rgb(\" + str(r) + \", \" + str(g) + \", \" + str(b) + \")\"\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, cv=None, n_jobs=-1, train_sizes=np.linspace(.008, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, \n",
    "                                                            X, \n",
    "                                                            y, \n",
    "                                                            cv=cv, \n",
    "                                                            n_jobs=n_jobs, \n",
    "                                                            train_sizes=train_sizes, \n",
    "                                                            scoring=\"f1\", \n",
    "                                                            random_state=RANDOM_SEED,\n",
    "                                                           )\n",
    "    \n",
    "    print(train_sizes, train_scores, test_scores)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    # Prints lower bound (mean - std) of train \n",
    "    trace1 = go.Scatter(\n",
    "        x=train_sizes, \n",
    "        y=train_scores_mean - train_scores_std, \n",
    "        showlegend=False,\n",
    "        mode=\"lines\",\n",
    "        name=\"\",\n",
    "        hoverlabel = dict(\n",
    "            namelength=20\n",
    "        ),\n",
    "        line = dict(\n",
    "            width = 0.1,\n",
    "            color = hexToRGBA(PLOTLY_COLORS[0], 0.4),\n",
    "        ),\n",
    "    )\n",
    "    # Prints upper bound (mean + std) of train\n",
    "    trace2 = go.Scatter(\n",
    "        x=train_sizes, \n",
    "        y=train_scores_mean + train_scores_std, \n",
    "        showlegend=False,\n",
    "        fill=\"tonexty\",\n",
    "        mode=\"lines\",\n",
    "        name=\"\",\n",
    "        hoverlabel = dict(\n",
    "            namelength=20\n",
    "        ),\n",
    "        line = dict(\n",
    "            width = 0.1,\n",
    "            color = hexToRGBA(PLOTLY_COLORS[0], 0.4),\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Prints mean train score line\n",
    "    trace3 = go.Scatter(\n",
    "        x=train_sizes, \n",
    "        y=train_scores_mean, \n",
    "        showlegend=True,\n",
    "        name=\"Train score\",\n",
    "        line = dict(\n",
    "            color = PLOTLY_COLORS[0],\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Prints lower bound (mean - std) of test \n",
    "    trace4 = go.Scatter(\n",
    "        x=train_sizes, \n",
    "        y=test_scores_mean - test_scores_std, \n",
    "        showlegend=False,\n",
    "        mode=\"lines\",\n",
    "        name=\"\",\n",
    "        hoverlabel = dict(\n",
    "            namelength=20\n",
    "        ),\n",
    "        line = dict(\n",
    "            width = 0.1,\n",
    "            color = hexToRGBA(PLOTLY_COLORS[1], 0.4),\n",
    "        ),\n",
    "    )\n",
    "        # Prints upper bound (mean + std) of test\n",
    "    trace5 = go.Scatter(\n",
    "        x=train_sizes, \n",
    "        y=test_scores_mean + test_scores_std, \n",
    "        showlegend=False,\n",
    "        fill=\"tonexty\",\n",
    "        mode=\"lines\",\n",
    "        name=\"\",\n",
    "        hoverlabel = dict(\n",
    "            namelength=20\n",
    "        ),\n",
    "        line = dict(\n",
    "            width = 0.1,\n",
    "            color = hexToRGBA(PLOTLY_COLORS[1], 0.4),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Prints mean test score line \n",
    "    trace6 = go.Scatter(\n",
    "        x=train_sizes, \n",
    "        y=test_scores_mean, \n",
    "        showlegend=True,\n",
    "        name=\"Test score\",\n",
    "        line = dict(\n",
    "            color = PLOTLY_COLORS[1],\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    data = [trace1, trace2, trace3, trace4, trace5, trace6]\n",
    "    layout = go.Layout(\n",
    "        title=title,\n",
    "        autosize=True,\n",
    "        yaxis=dict(\n",
    "            title='F1 Score',\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=\"#Training samples\",\n",
    "        ),\n",
    "        legend=dict(\n",
    "            x=0.8,\n",
    "            y=0,\n",
    "        ),\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    return iplot(fig, filename=title)\n",
    "\n",
    "\n",
    "def print_confusion_matrix(gs, X_test, y_test):\n",
    "\n",
    "    \"\"\"\n",
    "    Function that prints confusion matrix for a classifier\n",
    "    \n",
    "    :param (estimator) clf: Classifier object\n",
    "    :param (array-like) X_test: List of data to be tested with\n",
    "    :param (array-like) y_test: List of labels for test \n",
    "    \"\"\"  \n",
    "\n",
    "    gs_score = gs.score(X_test, y_test)\n",
    "    y_pred = gs.predict(X_test)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    t = PrettyTable()\n",
    "    t.add_row([\"True Sick\", cm[0][0], cm[0][1]])\n",
    "    t.add_row([\"True Healthy\", cm[1][0], cm[1][1]])\n",
    "    t.field_names = [\" \", \"Predicted Sick\", \"Predicted Healthy\"]\n",
    "    print(t)\n",
    "\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # normalize the confusion matrix\n",
    "    cm_df = pd.DataFrame(cm.round(3), index=[\"True Sick\", \"True Healthy\"], columns=[\"Predicted sick\", \"Predicted healthy\"])\n",
    "    cm_df\n",
    "\n",
    "\n",
    "def print_raw_score(clf, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Function that scores a classifier on some data\n",
    "    \n",
    "    :param (array of estimator) clf: Array of classifiers\n",
    "    :param (array-like) X_test: List of data to be tested with\n",
    "    :param (array-like) y_test: List of labels for test \n",
    "\n",
    "    \"\"\"  \n",
    "    print(\"Score achieved by NB: %0.3f\" % (score([clf], [(X_test, y_test)])[0]))\n",
    "\n",
    "\n",
    "def plot_feature_importance(feature_importance, title):\n",
    "    \"\"\"\n",
    "    Function that plots feature importance for a decision tree or a random forest classifier\n",
    "    \n",
    "    :param (dictionary) feature_importance: Dictionary of most important features sorted\n",
    "    :param (str) title: Title of the plot\n",
    "\n",
    "    \"\"\" \n",
    "    \n",
    "    trace1 = go.Bar(\n",
    "        x=feature_importance[:, 0],\n",
    "        y=feature_importance[:, 1],\n",
    "        marker = dict(color = PLOTLY_COLORS[0]),\n",
    "        opacity=PLOTLY_OPACITY,\n",
    "        name='Feature importance'\n",
    "    )\n",
    "    data = [trace1]\n",
    "    layout = go.Layout(\n",
    "        title=title,\n",
    "        autosize=True,\n",
    "        margin=go.layout.Margin(l=50, r=100, b=150),\n",
    "        xaxis=dict(\n",
    "            title='feature',\n",
    "            tickangle=30\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='feature importance',\n",
    "            automargin=True,\n",
    "        ),\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    return iplot(fig, filename=title)\n",
    "\n",
    "\n",
    "def print_performances(classifiers, classifier_names, auc_scores, X_test, y_test):\n",
    "  \n",
    "    \"\"\"\n",
    "    Function that scores a classifier on some data\n",
    "    \n",
    "    :param (array of estimator) clf: Array of classifiers\n",
    "    :param (array-like) classifier_names: Title of the classifier\n",
    "    :param (array-like) auc-score: Auc scores\n",
    "    :param (array-like) X_test: List of data to be tested with\n",
    "    :param (array-like) y_test: List of labels for test \n",
    "\n",
    "    \"\"\" \n",
    "\n",
    "    accs = []\n",
    "    recalls = []\n",
    "    precision = []\n",
    "    results_table = pd.DataFrame(columns=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"auc\"])\n",
    "    for (i, clf), name, auc in zip(enumerate(classifiers), classifier_names, auc_scores):\n",
    "        y_pred = clf.predict(X_test)\n",
    "        row = []\n",
    "        row.append(accuracy_score(y_test, y_pred))\n",
    "        row.append(precision_score(y_test, y_pred))\n",
    "        row.append(recall_score(y_test, y_pred))\n",
    "        row.append(f1_score(y_test, y_pred))\n",
    "        row.append(auc)\n",
    "        row = [\"%.3f\" % r for r in row]\n",
    "        results_table.loc[name] = row\n",
    "    return results_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5, random_state=RANDOM_SEED)\n",
    "clf_nb = GaussianNB()\n",
    "clf_knn = KNeighborsClassifier()\n",
    "clf_rf = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "clf_lr = LogisticRegression(random_state=RANDOM_SEED)\n",
    "clf_svm = SVC(random_state=RANDOM_SEED)\n",
    "\n",
    "clfs = [clf_nb, clf_knn, clf_rf, clf_lr, clf_svm]\n",
    "clfs_ng = [clf_nb, clf_rf]\n",
    "TEST_PARAMS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_results = []\n",
    "all_gss = []\n",
    "times = np.zeros(2)\n",
    "t = 0\n",
    "\n",
    "for clf in clfs:\n",
    "    gs_ohc_pc, t = param_tune_grid_cv(clf, TEST_PARAMS, X_train, y_train, kf, execution_time=True)\n",
    "    times[0] = times[0] + float(t)\n",
    "\n",
    "    gs_ohc, t = param_tune_grid_cv(clf, TEST_PARAMS, X_train_pca, y_train_pca, kf, execution_time=True)\n",
    "    times[1] = times[1] + float(t)\n",
    "\n",
    "    gss = [gs_ohc_pc, gs_ohc]\n",
    "    all_gss.append(gss)\n",
    "    test_results = score(gss, [(X_test, y_test),\n",
    "                                (X_test_pca, y_test_pca)])\n",
    "    all_test_results.append(test_results)\n",
    "\n",
    "all_test_results_ng = []\n",
    "all_gss_ng = []\n",
    "times_ng = np.zeros(3)\n",
    "\n",
    "for clf in clfs_ng:\n",
    "    gs_full, t = param_tune_grid_cv(clf, TEST_PARAMS, X_train, y_train, kf, execution_time=True)\n",
    "    times_ng[0] = times_ng[0] + float(t)\n",
    "  \n",
    "    #gs_pc, t = param_tune_grid_cv(clf, TEST_PARAMS, X_train_pc, y_train_pc, kf, execution_time=True)\n",
    "    #times[1] = times[1] + float(t)\n",
    "  \n",
    "    gs_drop, t = param_tune_grid_cv(clf, TEST_PARAMS, X_train_pca, y_train_pca, kf, execution_time=True)\n",
    "    times_ng[1] = times_ng[1] + float(t)\n",
    "  \n",
    "    #gs_pc_drop, t = param_tune_grid_cv(clf, TEST_PARAMS, X_train_pc_drop, y_train_pc_drop, kf, execution_time=True)\n",
    "    #times[3] = times[3] + float(t)\n",
    "  \n",
    "    #gs_no_stalk, t = param_tune_grid_cv(clf, TEST_PARAMS, X_train, y_train, kf, execution_time=True)\n",
    "    #times_ng[2] = times_ng[2] + float(t)\n",
    "\n",
    "    gss_ng = [gs_full, gs_drop, gs_no_stalk]\n",
    "    all_gss_ng.append(gss_ng)\n",
    "    test_results = score(gss_ng, [(X_test, y_test),\n",
    "                                  (X_test_pca, y_test_pca)\n",
    "                                 ])\n",
    "    all_test_results_ng.append(test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------------+------------------+\n",
      "|                     | dataset with scaled data | dataset with PCA |\n",
      "+---------------------+--------------------------+------------------+\n",
      "|    Random Forest    |          0.866           |      0.727       |\n",
      "| Logistic Regression |          0.853           |      0.853       |\n",
      "|         SVM         |          0.829           |      0.829       |\n",
      "|         KNN         |          0.800           |      0.800       |\n",
      "|     Naive Bayes     |          0.776           |      0.754       |\n",
      "+---------------------+--------------------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "def print_results(column_names, row_names, values):\n",
    "    \"\"\"\n",
    "    Function that prints a table using prettytable library\n",
    "    \n",
    "    :param (array) column_names: Array of column names\n",
    "    :param (array) row_names: Array of row names\n",
    "    :param (matrix) values: Values of the table\n",
    "\n",
    "    \"\"\" \n",
    "    t = PrettyTable()\n",
    "    t.field_names = column_names\n",
    "    \n",
    "    all_rows = []\n",
    "    result_row = []\n",
    "\n",
    "    for name, results in zip(row_names, values):\n",
    "        result_row.append(name)\n",
    "        for r in results:\n",
    "            result_row.append(\"%.3f\" % r)\n",
    "        all_rows.append(result_row)\n",
    "        result_row = []\n",
    "\n",
    "    all_rows = sorted(all_rows, key=lambda kv: kv[1], reverse=True)\n",
    "    for k in all_rows:\n",
    "        t.add_row(k)\n",
    "    \n",
    "    print(t)\n",
    "\n",
    "\n",
    "dataset_strings = [\" \",\n",
    "                   \"dataset with scaled data\",\n",
    "                   \"dataset with PCA\"]\n",
    "\n",
    "dataset_strings_ng = [\" \", \"full dataset\",\n",
    "                      \"dataset with dropped missing values\"]\n",
    "\n",
    "row_names = [\"Naive Bayes\", \"KNN\", \"Random Forest\", \"Logistic Regression\", \"SVM\"]\n",
    "#row_names_ng = [\"Naive Bayes\", \"Random Forest\"]\n",
    "\n",
    "print_results(dataset_strings, row_names, all_test_results)\n",
    "#print_results(dataset_strings_ng, row_names_ng, all_test_results_ng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+-------------------+\n",
      "|              | Predicted Sick | Predicted Healthy |\n",
      "+--------------+----------------+-------------------+\n",
      "|  True Sick   |       18       |         10        |\n",
      "| True Healthy |       7        |         26        |\n",
      "+--------------+----------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "print_confusion_matrix(all_gss_ng[0][np.argmin(all_test_results_ng[0])], X_test_pca, y_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------------------+------------------+\n",
      "|                        | dataset with scaled data | dataset with PCA |\n",
      "+------------------------+--------------------------+------------------+\n",
      "|  Total train time (s)  |          4.714           |      2.412       |\n",
      "| Mean score for dataset |          0.825           |      0.792       |\n",
      "+------------------------+--------------------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "means = np.mean(all_test_results, axis=0)\n",
    "row_names = [\"Total train time (s)\", \"Mean score for dataset\"]\n",
    "print_results(dataset_strings, row_names, [times, means])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset that gives the best overall performances is:\n",
      "\t- dataset with scaled data, with a score of 0.825\n"
     ]
    }
   ],
   "source": [
    "print(\"The dataset that gives the best overall performances is:\")\n",
    "print(\"\\t- \" + dataset_strings[means.argmax()+1] + \", with a score of \" + str(\"%.3f\" % means.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGISTIC REGRESSION\n",
    "\n",
    "Il primo classificatore che andiamo ad analizzare è la logistic regression: esso usa la funzione di Sigmoid per classificare i nostri sample:\n",
    "* $ P(y=0 | X;\\theta) = g(w^TX) = \\frac{1}{1+e^{w^TX}} $\n",
    "* $ P(y=1 | X;\\theta) = 1 - g(w^TX) = \\frac{e^{w^TX}}{1+e^{w^TX}} $\n",
    "\n",
    "Questo modello, rispetto alla linear regression, può modellare meglio la zona compresa tra 0 e 1. Per conoscere i pesi, bisogna calcolare la MLE (Maximum Likelihood Estimation, ossia scegliere un valore che massimizzi la probabilità dei dati osservati) ed applicare l'algoritmo di gradient descent fino alla convergenza del valore di accuratezza.\n",
    "\n",
    "Essi sono:\n",
    "* **liblinear**: Solver, migliore per dataset più piccoli.\n",
    "* **C**: \"forza di regolarizzazione\", valore compreso tra 0,01 e 100. Più è piccolo il valore, maggiore è la regolarizzazione\n",
    "* **penalty**: penalità \"l1\" e \"l2\" per la regolarizzazione, che sono definite come:\n",
    "    * l1, che penalizza ogni errore allo stesso modo $ S = \\sum_{i=1}^{n}| y_i - f(x_i) | $\n",
    "    * l2, che penalizza maggiormente valori più grandi $ S = \\sum_{i=1}^{n}(y_i - f(x_i))^2 $\n",
    "\n",
    "Dove $y_i$ è la vera label mentre $f(x_i)$ è quella assegnata.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best grid scores on validation set:\n",
      "+------------------+--------+--------------+-------------+\n",
      "|      Score       | clf__C | clf__penalty | clf__solver |\n",
      "+------------------+--------+--------------+-------------+\n",
      "| 0.868 (+/-0.032) |  0.1   |      l2      |    lbfgs    |\n",
      "| 0.868 (+/-0.032) |  0.1   |      l2      |  newton-cg  |\n",
      "| 0.867 (+/-0.031) |  0.1   |      l2      |  liblinear  |\n",
      "| 0.865 (+/-0.014) | 0.001  |      l2      |  liblinear  |\n",
      "| 0.863 (+/-0.010) |  0.01  |      l2      |  liblinear  |\n",
      "+------------------+--------+--------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "clf_lr = LogisticRegression(random_state=RANDOM_SEED)\n",
    "gs_pc_lr = param_tune_grid_cv(clf_lr, LOGISTIC_REGRESSION_PARAMS, X_train, y_train, kf)\n",
    "print_gridcv_scores(gs_pc_lr, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\Alberto\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"C:\\Users\\Alberto\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\Users\\Alberto\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 567, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\Alberto\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"C:\\Users\\Alberto\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"C:\\Users\\Alberto\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 528, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Alberto\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 267, in fit\n    self._final_estimator.fit(Xt, y, **fit_params)\n  File \"C:\\Users\\Alberto\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\", line 1319, in fit\n    \" class: %r\" % classes_[0])\nValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-2defa0f0330e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m                     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                     \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m                    )\n",
      "\u001b[1;32m<ipython-input-97-92dab7e9399a>\u001b[0m in \u001b[0;36mplot_learning_curve\u001b[1;34m(estimator, title, X, y, cv, n_jobs, train_sizes)\u001b[0m\n\u001b[0;32m    142\u001b[0m                                                             \u001b[0mtrain_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_sizes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                                                             \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"f1\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m                                                             \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRANDOM_SEED\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m                                                            )\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mlearning_curve\u001b[1;34m(estimator, X, y, groups, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose, shuffle, random_state, error_score)\u001b[0m\n\u001b[0;32m   1244\u001b[0m             \u001b[0mparameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m             error_score=error_score)\n\u001b[1;32m-> 1246\u001b[1;33m             for train, test in train_test_proportions)\n\u001b[0m\u001b[0;32m   1247\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m         \u001b[0mn_cv_folds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mn_unique_ticks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1"
     ]
    }
   ],
   "source": [
    "plot_learning_curve(gs_pc_lr.best_estimator_, \"Learning curve of Logistic Regression\", \n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    cv=5,\n",
    "                   )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
